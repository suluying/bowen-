{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "#ppi_matrix=sp.load_npz('./data/ppi_matrix7132.npz')\n",
    "all_sequence_outputsnew=np.load('./data/all_sequence_outputs7132.npy')\n",
    "merged_df=pd.read_csv('./data/merged_df7132.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个蛋白对于多个transcript，这部分代码是基于蛋白的ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41441,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7132x7132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 364726 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ppi直接加载构建好的npz，以下是构建npz步骤\n",
    "ppi = pd.read_csv('./data/ppi3ensp.csv')\n",
    "print(ppi.protein_id2.unique().shape)\n",
    "gene_id_dict = {gene: i for i, gene in enumerate(merged_df['protein'])}\n",
    "#将ppi中的Protein1变为protein_id1在gene_id_dict中对应的序号\n",
    "ppi['Protein1'] = ppi['protein_id1'].map(gene_id_dict)\n",
    "#将ppi中的Protein2变为protein_id2在gene_id_dict中对应的序号\n",
    "ppi['Protein2'] = ppi['protein_id2'].map(gene_id_dict)\n",
    "ppi=ppi.dropna()\n",
    "#用ppi构建一个coo的稀疏举证，用protein1和protein2构建一个coo的稀疏矩阵\n",
    "ppi_matrix = sp.coo_matrix((ppi[\"CombinedScore\"], (ppi['Protein1'], ppi['Protein2'])), shape=(merged_df.shape[0], merged_df.shape[0]))\n",
    "\n",
    "ppi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7132, 7132)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj=ppi_matrix.todense()\n",
    "adj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加图之前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 5\n",
      "sequence_embedding shape: torch.Size([7132, 9216])\n",
      "Expected sequence_embedding shape to be (N, 9126), but got torch.Size([7132, 9216])\n",
      "Total Loss: 2063767.635055693, Epoch: 0, R²: 0.0285\n",
      "Total Loss: 1392380.1432549504, Epoch: 1, R²: 0.2854\n",
      "Total Loss: 1205721.994740099, Epoch: 2, R²: 0.3584\n",
      "Total Loss: 1138004.0949876239, Epoch: 3, R²: 0.4019\n",
      "Total Loss: 1111101.6893564356, Epoch: 4, R²: 0.4267\n",
      "Total Loss: 1099185.6492883663, Epoch: 5, R²: 0.4356\n",
      "Total Loss: 1091923.3106435644, Epoch: 6, R²: 0.4442\n",
      "Total Loss: 1107293.6551670793, Epoch: 7, R²: 0.4454\n",
      "Total Loss: 1195094.6649133663, Epoch: 8, R²: 0.4481\n",
      "Total Loss: 1088374.1629022278, Epoch: 9, R²: 0.4497\n",
      "Total Loss: 1083429.2586633663, Epoch: 10, R²: 0.4519\n",
      "Total Loss: 1081404.3058477722, Epoch: 11, R²: 0.4486\n",
      "Total Loss: 1084103.0238242573, Epoch: 12, R²: 0.4549\n",
      "Total Loss: 1116282.0287747525, Epoch: 13, R²: 0.4552\n",
      "Total Loss: 1090327.609839109, Epoch: 14, R²: 0.4545\n",
      "Total Loss: 1077519.6036509902, Epoch: 15, R²: 0.4585\n",
      "Total Loss: 1086654.6577970297, Epoch: 16, R²: 0.4582\n",
      "Total Loss: 1074394.8319152228, Epoch: 17, R²: 0.4522\n",
      "Total Loss: 1076788.4260519801, Epoch: 18, R²: 0.4566\n",
      "Total Loss: 1074904.0541460395, Epoch: 19, R²: 0.4588\n",
      "Total Loss: 1069002.5671410891, Epoch: 20, R²: 0.4604\n",
      "Total Loss: 1091518.9556002475, Epoch: 21, R²: 0.4598\n",
      "Total Loss: 1067999.2527846536, Epoch: 22, R²: 0.4623\n",
      "Total Loss: 1065906.6998762377, Epoch: 23, R²: 0.4625\n",
      "Total Loss: 1061545.3555074257, Epoch: 24, R²: 0.4644\n",
      "Total Loss: 1054856.379950495, Epoch: 25, R²: 0.4668\n",
      "Total Loss: 1053153.1633663366, Epoch: 26, R²: 0.4665\n",
      "Total Loss: 1050341.5976175743, Epoch: 27, R²: 0.4683\n",
      "Total Loss: 1056492.4504950496, Epoch: 28, R²: 0.4667\n",
      "Total Loss: 1041576.7439665842, Epoch: 29, R²: 0.4721\n",
      "Total Loss: 1045565.2267945545, Epoch: 30, R²: 0.4709\n",
      "Total Loss: 1041657.2959467822, Epoch: 31, R²: 0.4710\n",
      "Total Loss: 1033573.0354269802, Epoch: 32, R²: 0.4762\n",
      "Total Loss: 1030228.4549814357, Epoch: 33, R²: 0.4780\n",
      "Total Loss: 1037422.4987623763, Epoch: 34, R²: 0.4754\n",
      "Total Loss: 1040731.9668935643, Epoch: 35, R²: 0.4784\n",
      "Total Loss: 1029564.4863861386, Epoch: 36, R²: 0.4763\n",
      "Total Loss: 1022063.4651918317, Epoch: 37, R²: 0.4812\n",
      "Total Loss: 1020018.7756806931, Epoch: 38, R²: 0.4817\n",
      "Total Loss: 1014720.0922029703, Epoch: 39, R²: 0.4854\n",
      "Total Loss: 1014414.3194616337, Epoch: 40, R²: 0.4844\n",
      "Total Loss: 1010131.9135210396, Epoch: 41, R²: 0.4864\n",
      "Total Loss: 1010151.2091584158, Epoch: 42, R²: 0.4859\n",
      "Total Loss: 1015608.7775371287, Epoch: 43, R²: 0.4852\n",
      "Total Loss: 1017973.0853960396, Epoch: 44, R²: 0.4910\n",
      "Total Loss: 1014539.9514232674, Epoch: 45, R²: 0.4928\n",
      "Total Loss: 1005181.3603032178, Epoch: 46, R²: 0.4915\n",
      "Total Loss: 996126.3217821782, Epoch: 47, R²: 0.4865\n",
      "Total Loss: 996797.1802289604, Epoch: 48, R²: 0.4872\n",
      "Total Loss: 1005922.5430074257, Epoch: 49, R²: 0.4910\n",
      "Total Loss: 1006540.948019802, Epoch: 50, R²: 0.4927\n",
      "Total Loss: 988715.3487004951, Epoch: 51, R²: 0.4932\n",
      "Total Loss: 985718.8685024752, Epoch: 52, R²: 0.4923\n",
      "Total Loss: 993803.9248143565, Epoch: 53, R²: 0.4937\n",
      "Total Loss: 990055.7065284654, Epoch: 54, R²: 0.4986\n",
      "Total Loss: 977784.0289294554, Epoch: 55, R²: 0.4992\n",
      "Total Loss: 979174.5682240099, Epoch: 56, R²: 0.4989\n",
      "Total Loss: 983657.2947091584, Epoch: 57, R²: 0.4954\n",
      "Total Loss: 974521.4641089109, Epoch: 58, R²: 0.5023\n",
      "Total Loss: 973982.6765160891, Epoch: 59, R²: 0.5019\n",
      "Total Loss: 978611.5925123763, Epoch: 60, R²: 0.5006\n",
      "Total Loss: 976883.6073638614, Epoch: 61, R²: 0.5023\n",
      "Total Loss: 965412.4931930694, Epoch: 62, R²: 0.5059\n",
      "Total Loss: 967008.2800123763, Epoch: 63, R²: 0.5048\n",
      "Total Loss: 972071.3361695545, Epoch: 64, R²: 0.5062\n",
      "Total Loss: 960642.1205136139, Epoch: 65, R²: 0.5062\n",
      "Total Loss: 961027.5809096535, Epoch: 66, R²: 0.5082\n",
      "Total Loss: 958657.7957920792, Epoch: 67, R²: 0.5114\n",
      "Total Loss: 956033.9382735149, Epoch: 68, R²: 0.5094\n",
      "Total Loss: 950169.5703898515, Epoch: 69, R²: 0.5094\n",
      "Total Loss: 950159.2447400991, Epoch: 70, R²: 0.5109\n",
      "Total Loss: 976530.9000618812, Epoch: 71, R²: 0.5106\n",
      "Total Loss: 951578.8578279703, Epoch: 72, R²: 0.5097\n",
      "Total Loss: 946986.4873143565, Epoch: 73, R²: 0.5129\n",
      "Total Loss: 946169.0875618812, Epoch: 74, R²: 0.5117\n",
      "Total Loss: 950832.0448638614, Epoch: 75, R²: 0.5163\n",
      "Total Loss: 939249.6942295792, Epoch: 76, R²: 0.5149\n",
      "Total Loss: 941729.2985767326, Epoch: 77, R²: 0.5180\n",
      "Total Loss: 938920.2091584158, Epoch: 78, R²: 0.5123\n",
      "Total Loss: 939015.1398514851, Epoch: 79, R²: 0.5179\n",
      "Total Loss: 940390.0126856435, Epoch: 80, R²: 0.5163\n",
      "Total Loss: 932673.0151608911, Epoch: 81, R²: 0.5152\n",
      "Total Loss: 931033.3455290842, Epoch: 82, R²: 0.5170\n",
      "Total Loss: 938813.5074257426, Epoch: 83, R²: 0.5203\n",
      "Total Loss: 929910.5795173268, Epoch: 84, R²: 0.5206\n",
      "Total Loss: 933446.6319616337, Epoch: 85, R²: 0.5185\n",
      "Total Loss: 927991.4775680694, Epoch: 86, R²: 0.5187\n",
      "Total Loss: 924747.3615408416, Epoch: 87, R²: 0.5201\n",
      "Total Loss: 924752.7076113861, Epoch: 88, R²: 0.5193\n",
      "Total Loss: 927146.4337871287, Epoch: 89, R²: 0.5189\n",
      "Total Loss: 921899.6582611386, Epoch: 90, R²: 0.5226\n",
      "Total Loss: 923203.8951113861, Epoch: 91, R²: 0.5217\n",
      "Total Loss: 922982.3431311881, Epoch: 92, R²: 0.5220\n",
      "Total Loss: 923031.4546720297, Epoch: 93, R²: 0.5191\n",
      "Total Loss: 917553.8318378713, Epoch: 94, R²: 0.5219\n",
      "Total Loss: 918420.9992264851, Epoch: 95, R²: 0.5239\n",
      "Total Loss: 913759.0929764851, Epoch: 96, R²: 0.5201\n",
      "Total Loss: 912387.6571782178, Epoch: 97, R²: 0.5223\n",
      "Total Loss: 914367.4611695545, Epoch: 98, R²: 0.5182\n",
      "Total Loss: 913019.5956064357, Epoch: 99, R²: 0.5206\n",
      "Total Loss: 914295.9992264851, Epoch: 100, R²: 0.5232\n",
      "Total Loss: 911071.6574876237, Epoch: 101, R²: 0.5255\n",
      "Total Loss: 914296.661355198, Epoch: 102, R²: 0.5241\n",
      "Total Loss: 906412.2066831683, Epoch: 103, R²: 0.5258\n",
      "Total Loss: 906335.4535891089, Epoch: 104, R²: 0.5222\n",
      "Total Loss: 900998.7143409654, Epoch: 105, R²: 0.5284\n",
      "Total Loss: 905281.6212871287, Epoch: 106, R²: 0.5272\n",
      "Total Loss: 906471.5646658416, Epoch: 107, R²: 0.5207\n",
      "Total Loss: 908507.0697710396, Epoch: 108, R²: 0.5225\n",
      "Total Loss: 900147.7201423268, Epoch: 109, R²: 0.5194\n",
      "Total Loss: 910466.3242574257, Epoch: 110, R²: 0.5266\n",
      "Total Loss: 898068.0281559406, Epoch: 111, R²: 0.5250\n",
      "Total Loss: 898894.5216584158, Epoch: 112, R²: 0.5291\n",
      "Total Loss: 895912.9569925743, Epoch: 113, R²: 0.5239\n",
      "Total Loss: 893257.4424504951, Epoch: 114, R²: 0.5304\n",
      "Total Loss: 900258.1882735149, Epoch: 115, R²: 0.5298\n",
      "Total Loss: 894603.973545792, Epoch: 116, R²: 0.5273\n",
      "Total Loss: 891826.3053836634, Epoch: 117, R²: 0.5246\n",
      "Total Loss: 899921.067605198, Epoch: 118, R²: 0.5268\n",
      "Total Loss: 889625.2142636139, Epoch: 119, R²: 0.5293\n",
      "Total Loss: 889068.6192759901, Epoch: 120, R²: 0.5306\n",
      "Total Loss: 890277.0447091584, Epoch: 121, R²: 0.5291\n",
      "Total Loss: 893950.5491955446, Epoch: 122, R²: 0.5280\n",
      "Total Loss: 885461.8115717822, Epoch: 123, R²: 0.5295\n",
      "Total Loss: 885132.8106435643, Epoch: 124, R²: 0.5258\n",
      "Total Loss: 884395.8635519802, Epoch: 125, R²: 0.5259\n",
      "Total Loss: 888122.9098081683, Epoch: 126, R²: 0.5285\n",
      "Total Loss: 887014.3316831683, Epoch: 127, R²: 0.5234\n",
      "Total Loss: 882785.6698638614, Epoch: 128, R²: 0.5272\n",
      "Total Loss: 901120.7837252475, Epoch: 129, R²: 0.5308\n",
      "Total Loss: 881323.1834777228, Epoch: 130, R²: 0.5226\n",
      "Total Loss: 896908.0776608911, Epoch: 131, R²: 0.5291\n",
      "Total Loss: 875656.9904084158, Epoch: 132, R²: 0.5258\n",
      "Total Loss: 881187.317914604, Epoch: 133, R²: 0.5279\n",
      "Total Loss: 879334.5575495049, Epoch: 134, R²: 0.5271\n",
      "Total Loss: 896173.588954208, Epoch: 135, R²: 0.5268\n",
      "Total Loss: 876056.5027846535, Epoch: 136, R²: 0.5301\n",
      "Total Loss: 873695.9460086634, Epoch: 137, R²: 0.5297\n",
      "Total Loss: 926879.3400371287, Epoch: 138, R²: 0.5239\n",
      "Total Loss: 871165.1825495049, Epoch: 139, R²: 0.5293\n",
      "Total Loss: 875163.6126237623, Epoch: 140, R²: 0.5264\n",
      "Total Loss: 891459.2612933168, Epoch: 141, R²: 0.5275\n",
      "Total Loss: 873351.1268564357, Epoch: 142, R²: 0.5247\n",
      "Total Loss: 868907.1055074257, Epoch: 143, R²: 0.5238\n",
      "Total Loss: 870542.1319616337, Epoch: 144, R²: 0.5271\n",
      "Total Loss: 882497.0382116337, Epoch: 145, R²: 0.5245\n",
      "Total Loss: 866405.2209158416, Epoch: 146, R²: 0.5305\n",
      "Total Loss: 868350.6302599009, Epoch: 147, R²: 0.5254\n",
      "Total Loss: 865106.0187190594, Epoch: 148, R²: 0.5294\n",
      "Total Loss: 863256.6902846535, Epoch: 149, R²: 0.5224\n",
      "Total Loss: 864117.3524133663, Epoch: 150, R²: 0.5272\n",
      "Total Loss: 862957.6859529703, Epoch: 151, R²: 0.5250\n",
      "Total Loss: 862315.2430383663, Epoch: 152, R²: 0.5272\n",
      "Total Loss: 865482.359684406, Epoch: 153, R²: 0.5296\n",
      "Total Loss: 861869.9347153465, Epoch: 154, R²: 0.5281\n",
      "Total Loss: 866378.536355198, Epoch: 155, R²: 0.5291\n",
      "Total Loss: 858526.7792388614, Epoch: 156, R²: 0.5229\n",
      "Total Loss: 854626.3681930694, Epoch: 157, R²: 0.5272\n",
      "Total Loss: 857003.5462561881, Epoch: 158, R²: 0.5233\n",
      "Total Loss: 854078.8934096535, Epoch: 159, R²: 0.5219\n",
      "Total Loss: 854452.2667079208, Epoch: 160, R²: 0.5279\n",
      "Total Loss: 864279.3763923268, Epoch: 161, R²: 0.5264\n",
      "Total Loss: 855952.9820544554, Epoch: 162, R²: 0.5233\n",
      "Total Loss: 856664.3898514851, Epoch: 163, R²: 0.5250\n",
      "Total Loss: 877327.948019802, Epoch: 164, R²: 0.5248\n",
      "Total Loss: 851317.4410581683, Epoch: 165, R²: 0.5247\n",
      "Total Loss: 851053.155940594, Epoch: 166, R²: 0.5278\n",
      "Total Loss: 849572.9681311881, Epoch: 167, R²: 0.5249\n",
      "Total Loss: 846899.0095915842, Epoch: 168, R²: 0.5276\n",
      "Total Loss: 849417.3112623763, Epoch: 169, R²: 0.5237\n",
      "Total Loss: 844351.0379022277, Epoch: 170, R²: 0.5238\n",
      "Total Loss: 847468.7128712871, Epoch: 171, R²: 0.5266\n",
      "Total Loss: 847920.5317141089, Epoch: 172, R²: 0.5271\n",
      "Total Loss: 849176.7301980198, Epoch: 173, R²: 0.5203\n",
      "Total Loss: 841396.3177599009, Epoch: 174, R²: 0.5240\n",
      "Total Loss: 839591.9682858911, Epoch: 175, R²: 0.5222\n",
      "Total Loss: 837117.7922339109, Epoch: 176, R²: 0.5231\n",
      "Total Loss: 841568.3066212871, Epoch: 177, R²: 0.5247\n",
      "Total Loss: 840177.0052599009, Epoch: 178, R²: 0.5246\n",
      "Total Loss: 840067.9436881188, Epoch: 179, R²: 0.5215\n",
      "Total Loss: 836274.5714727723, Epoch: 180, R²: 0.5229\n",
      "Total Loss: 835558.5696163366, Epoch: 181, R²: 0.5231\n",
      "Total Loss: 837337.0512066832, Epoch: 182, R²: 0.5238\n",
      "Total Loss: 837295.4914526609, Epoch: 183, R²: 0.5244\n",
      "Total Loss: 832993.5533725248, Epoch: 184, R²: 0.5243\n",
      "Total Loss: 830582.2018873763, Epoch: 185, R²: 0.5172\n",
      "Total Loss: 833406.6881961634, Epoch: 186, R²: 0.5216\n",
      "Total Loss: 831509.9464727723, Epoch: 187, R²: 0.5215\n",
      "Total Loss: 830930.926980198, Epoch: 188, R²: 0.5235\n",
      "Total Loss: 830632.3293626237, Epoch: 189, R²: 0.5217\n",
      "Total Loss: 833506.9698329208, Epoch: 190, R²: 0.5170\n",
      "Total Loss: 829783.4724628713, Epoch: 191, R²: 0.5206\n",
      "Total Loss: 831467.9959777228, Epoch: 192, R²: 0.5220\n",
      "Total Loss: 827516.3406559406, Epoch: 193, R²: 0.5203\n",
      "Total Loss: 824412.9286045792, Epoch: 194, R²: 0.5198\n",
      "Total Loss: 824437.7993502475, Epoch: 195, R²: 0.5186\n",
      "Total Loss: 826436.1608910891, Epoch: 196, R²: 0.5219\n",
      "Total Loss: 823147.4574566832, Epoch: 197, R²: 0.5198\n",
      "Total Loss: 824394.114170792, Epoch: 198, R²: 0.5193\n",
      "Total Loss: 826511.896039604, Epoch: 199, R²: 0.5215\n",
      "Total Loss: 822897.5725556931, Epoch: 200, R²: 0.5214\n",
      "Total Loss: 824823.3677289604, Epoch: 201, R²: 0.5193\n",
      "Total Loss: 854275.9064047029, Epoch: 202, R²: 0.5194\n",
      "Total Loss: 820376.0379022277, Epoch: 203, R²: 0.5121\n",
      "Total Loss: 816069.2410272277, Epoch: 204, R²: 0.5188\n",
      "Total Loss: 819055.1383044554, Epoch: 205, R²: 0.5127\n",
      "Total Loss: 824169.5241336634, Epoch: 206, R²: 0.5108\n",
      "Total Loss: 818788.4803527228, Epoch: 207, R²: 0.5182\n",
      "Total Loss: 816867.5770420792, Epoch: 208, R²: 0.5135\n",
      "Total Loss: 814430.3767017326, Epoch: 209, R²: 0.5142\n",
      "Total Loss: 813927.2297339109, Epoch: 210, R²: 0.5162\n",
      "Total Loss: 811919.151144802, Epoch: 211, R²: 0.5170\n",
      "Total Loss: 816350.755105198, Epoch: 212, R²: 0.5080\n",
      "Total Loss: 809063.8620049505, Epoch: 213, R²: 0.5178\n",
      "Total Loss: 807890.4563737623, Epoch: 214, R²: 0.5189\n",
      "Total Loss: 809666.7614480198, Epoch: 215, R²: 0.5134\n",
      "Total Loss: 818456.4212561881, Epoch: 216, R²: 0.5169\n",
      "Total Loss: 805700.3018254951, Epoch: 217, R²: 0.5160\n",
      "Total Loss: 805894.198019802, Epoch: 218, R²: 0.5152\n",
      "Total Loss: 807793.2783106435, Epoch: 219, R²: 0.5152\n",
      "Total Loss: 809147.5221225248, Epoch: 220, R²: 0.5179\n",
      "Total Loss: 802705.3777846535, Epoch: 221, R²: 0.5129\n",
      "Total Loss: 805201.6785272277, Epoch: 222, R²: 0.5153\n",
      "Total Loss: 815410.6577970297, Epoch: 223, R²: 0.5121\n",
      "Total Loss: 802332.1398514851, Epoch: 224, R²: 0.5159\n",
      "Total Loss: 802097.1638304455, Epoch: 225, R²: 0.5142\n",
      "Total Loss: 801572.5225866337, Epoch: 226, R²: 0.5164\n",
      "Total Loss: 797936.1947710396, Epoch: 227, R²: 0.5143\n",
      "Total Loss: 815602.5645111386, Epoch: 228, R²: 0.5127\n",
      "Total Loss: 796890.3162128713, Epoch: 229, R²: 0.5151\n",
      "Total Loss: 796944.3168316832, Epoch: 230, R²: 0.5121\n",
      "Total Loss: 797066.5457920792, Epoch: 231, R²: 0.5099\n",
      "Total Loss: 804023.2159653465, Epoch: 232, R²: 0.5136\n",
      "Total Loss: 805744.088644802, Epoch: 233, R²: 0.5090\n",
      "Total Loss: 791100.3680383663, Epoch: 234, R²: 0.5169\n",
      "Total Loss: 794814.0806002475, Epoch: 235, R²: 0.5097\n",
      "Total Loss: 793029.0139232674, Epoch: 236, R²: 0.5114\n",
      "Total Loss: 796452.9009900991, Epoch: 237, R²: 0.5116\n",
      "Total Loss: 791046.4402846535, Epoch: 238, R²: 0.5109\n",
      "Total Loss: 790489.921875, Epoch: 239, R²: 0.5141\n",
      "Total Loss: 788423.5592512377, Epoch: 240, R²: 0.5090\n",
      "Total Loss: 785867.736772896, Epoch: 241, R²: 0.5126\n",
      "Total Loss: 790564.4798886139, Epoch: 242, R²: 0.5141\n",
      "Total Loss: 787320.7540222772, Epoch: 243, R²: 0.5125\n",
      "Total Loss: 787825.9975247525, Epoch: 244, R²: 0.5126\n",
      "Total Loss: 784432.5027846535, Epoch: 245, R²: 0.5141\n",
      "Total Loss: 780914.8144337871, Epoch: 246, R²: 0.5106\n",
      "Total Loss: 783041.6222153465, Epoch: 247, R²: 0.5071\n",
      "Total Loss: 778427.3090965346, Epoch: 248, R²: 0.5124\n",
      "Total Loss: 782839.7530940594, Epoch: 249, R²: 0.5117\n",
      "Total Loss: 787304.7399443069, Epoch: 250, R²: 0.5124\n",
      "Total Loss: 787478.0078898515, Epoch: 251, R²: 0.5126\n",
      "Total Loss: 786815.8579826732, Epoch: 252, R²: 0.5099\n",
      "Total Loss: 778484.6916769802, Epoch: 253, R²: 0.5139\n",
      "Total Loss: 777503.4455445545, Epoch: 254, R²: 0.5106\n",
      "Total Loss: 776801.2826423268, Epoch: 255, R²: 0.5091\n",
      "Total Loss: 793131.2693378713, Epoch: 256, R²: 0.5122\n",
      "Total Loss: 777824.4588490099, Epoch: 257, R²: 0.5108\n",
      "Total Loss: 773501.2181311881, Epoch: 258, R²: 0.5123\n",
      "Total Loss: 776394.2916150991, Epoch: 259, R²: 0.5122\n",
      "Total Loss: 775353.2337561881, Epoch: 260, R²: 0.5081\n",
      "Total Loss: 771771.5413056931, Epoch: 261, R²: 0.5102\n",
      "Total Loss: 772591.8879950495, Epoch: 262, R²: 0.5098\n",
      "Total Loss: 768975.083539604, Epoch: 263, R²: 0.5032\n",
      "Total Loss: 773140.0741027228, Epoch: 264, R²: 0.5095\n",
      "Total Loss: 768500.1383044554, Epoch: 265, R²: 0.5103\n",
      "Total Loss: 764351.364480198, Epoch: 266, R²: 0.5100\n",
      "Total Loss: 766660.6709467822, Epoch: 267, R²: 0.5087\n",
      "Total Loss: 766549.8785581683, Epoch: 268, R²: 0.5095\n",
      "Total Loss: 764546.5071163366, Epoch: 269, R²: 0.5074\n",
      "Total Loss: 776373.5558477723, Epoch: 270, R²: 0.5050\n",
      "Total Loss: 765858.2807858911, Epoch: 271, R²: 0.5089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m pair_emb \u001b[38;5;241m=\u001b[39m pair_emb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     95\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 97\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mneural_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y, y_pred)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, x, sequence_embedding, paired_ratio)\u001b[0m\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m x_out  \u001b[38;5;241m+\u001b[39m seq_out\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mcat((paired_out,x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from src.utils import set_seed\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 5\n",
    "set_seed(SEED)\n",
    "\n",
    "# 确保 merged_df 和 all_sequence_outputsnew 已定义\n",
    "X = torch.tensor(np.array(merged_df['rNC2'], dtype=np.float32).reshape(-1, 1))\n",
    "y = torch.tensor(np.array(merged_df['NC3'], dtype=np.float32).reshape(-1, 1))\n",
    "paired_ratio = torch.tensor(np.array(merged_df['High_Pause_Counts'], dtype=np.float32).reshape(-1, 1))\n",
    "sequence_embedding = torch.tensor(all_sequence_outputsnew, dtype=torch.float32)\n",
    "\n",
    "# 检查 sequence_embedding 的形状\n",
    "print(f'sequence_embedding shape: {sequence_embedding.shape}')\n",
    "\n",
    "# 假设 sequence_embedding 的第二个维度应该是 9126，根据错误信息进行修正\n",
    "# 如果实际的 shape 不是 9126，请根据实际情况进行调整\n",
    "if sequence_embedding.shape[1] != 9126:\n",
    "    print(f'Expected sequence_embedding shape to be (N, 9126), but got {sequence_embedding.shape}')\n",
    "\n",
    "# 划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)\n",
    "sequence_embedding_train, sequence_embedding_test = train_test_split(sequence_embedding, test_size=0.1, random_state=SEED)\n",
    "paired_ratio_train, paired_ratio_test = train_test_split(paired_ratio, test_size=0.1, random_state=SEED)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = TensorDataset(X_train, sequence_embedding_train, paired_ratio_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, sequence_embedding_test, paired_ratio_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc_x = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.fc_paired = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9216, 32),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sequence_embedding, paired_ratio):\n",
    "        x_out = self.fc_x(x)\n",
    "        paired_out = self.fc_paired(paired_ratio)\n",
    "        seq_out = self.encoder(sequence_embedding)\n",
    "        \n",
    "        x = x_out  + seq_out\n",
    "        x =  torch.cat((paired_out,x), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net = NeuralNet().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=3e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 100000  # 为了示例，使用更少的epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    neural_net.train()\n",
    "    total_loss_epoch = 0\n",
    "\n",
    "    for x, seq_emb, pair_emb, y in train_loader:  # 使用批量数据\n",
    "\n",
    "        x = x.to(device)\n",
    "        seq_emb = seq_emb.to(device)\n",
    "        pair_emb = pair_emb.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = neural_net(x, seq_emb, pair_emb)\n",
    "\n",
    "        loss = criterion(y, y_pred)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss_epoch += loss.item()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        neural_net.eval()  # Set the neural_net to evaluation mode\n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            X_test = X_test.to(device)\n",
    "            sequence_embedding_test = sequence_embedding_test.to(device)\n",
    "            paired_ratio_test = paired_ratio_test.to(device)\n",
    "            \n",
    "            y_pred = neural_net(X_test, sequence_embedding_test, paired_ratio_test).cpu().numpy()\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "        print(f'Total Loss: {total_loss_epoch/len(train_loader)}, Epoch: {epoch}, R²: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 5\n",
      "Epoch: 001, Train Loss: 2342620.750, Train R²: -0.357, Test Loss: 2282977.000, Test R²: -0.379\n",
      "Epoch: 002, Train Loss: 2328652.750, Train R²: -0.349, Test Loss: 2269429.500, Test R²: -0.371\n",
      "Epoch: 003, Train Loss: 2314751.750, Train R²: -0.341, Test Loss: 2255956.000, Test R²: -0.363\n",
      "Epoch: 004, Train Loss: 2300677.500, Train R²: -0.333, Test Loss: 2242305.750, Test R²: -0.355\n",
      "Epoch: 005, Train Loss: 2286431.250, Train R²: -0.325, Test Loss: 2228492.000, Test R²: -0.346\n",
      "Epoch: 006, Train Loss: 2271661.750, Train R²: -0.316, Test Loss: 2214159.750, Test R²: -0.338\n",
      "Epoch: 007, Train Loss: 2256204.000, Train R²: -0.307, Test Loss: 2199139.500, Test R²: -0.329\n",
      "Epoch: 008, Train Loss: 2239956.750, Train R²: -0.298, Test Loss: 2183339.250, Test R²: -0.319\n",
      "Epoch: 009, Train Loss: 2222829.500, Train R²: -0.288, Test Loss: 2166664.500, Test R²: -0.309\n",
      "Epoch: 010, Train Loss: 2204700.500, Train R²: -0.277, Test Loss: 2149006.500, Test R²: -0.298\n",
      "Epoch: 011, Train Loss: 2185315.750, Train R²: -0.266, Test Loss: 2130052.000, Test R²: -0.287\n",
      "Epoch: 012, Train Loss: 2164686.500, Train R²: -0.254, Test Loss: 2109849.000, Test R²: -0.275\n",
      "Epoch: 013, Train Loss: 2142547.750, Train R²: -0.241, Test Loss: 2088149.750, Test R²: -0.262\n",
      "Epoch: 014, Train Loss: 2118888.750, Train R²: -0.227, Test Loss: 2064936.875, Test R²: -0.248\n",
      "Epoch: 015, Train Loss: 2093784.500, Train R²: -0.213, Test Loss: 2040283.750, Test R²: -0.233\n",
      "Epoch: 016, Train Loss: 2067182.875, Train R²: -0.198, Test Loss: 2014133.000, Test R²: -0.217\n",
      "Epoch: 017, Train Loss: 2039028.125, Train R²: -0.181, Test Loss: 1986444.750, Test R²: -0.200\n",
      "Epoch: 018, Train Loss: 2009265.875, Train R²: -0.164, Test Loss: 1957154.625, Test R²: -0.183\n",
      "Epoch: 019, Train Loss: 1977862.125, Train R²: -0.146, Test Loss: 1926216.875, Test R²: -0.164\n",
      "Epoch: 020, Train Loss: 1944764.125, Train R²: -0.127, Test Loss: 1893572.750, Test R²: -0.144\n",
      "Epoch: 021, Train Loss: 1909888.375, Train R²: -0.106, Test Loss: 1859135.375, Test R²: -0.123\n",
      "Epoch: 022, Train Loss: 1873311.000, Train R²: -0.085, Test Loss: 1822983.375, Test R²: -0.101\n",
      "Epoch: 023, Train Loss: 1835190.750, Train R²: -0.063, Test Loss: 1785258.500, Test R²: -0.079\n",
      "Epoch: 024, Train Loss: 1795682.750, Train R²: -0.040, Test Loss: 1746121.000, Test R²: -0.055\n",
      "Epoch: 025, Train Loss: 1754969.625, Train R²: -0.017, Test Loss: 1705705.125, Test R²: -0.031\n",
      "Epoch: 026, Train Loss: 1713265.625, Train R²: 0.007, Test Loss: 1664248.125, Test R²: -0.006\n",
      "Epoch: 027, Train Loss: 1670730.125, Train R²: 0.032, Test Loss: 1621906.125, Test R²: 0.020\n",
      "Epoch: 028, Train Loss: 1627420.625, Train R²: 0.057, Test Loss: 1578702.750, Test R²: 0.046\n",
      "Epoch: 029, Train Loss: 1584412.125, Train R²: 0.082, Test Loss: 1535721.875, Test R²: 0.072\n",
      "Epoch: 030, Train Loss: 1542347.500, Train R²: 0.107, Test Loss: 1493601.250, Test R²: 0.098\n",
      "Epoch: 031, Train Loss: 1502174.250, Train R²: 0.130, Test Loss: 1453238.750, Test R²: 0.122\n",
      "Epoch: 032, Train Loss: 1464812.750, Train R²: 0.151, Test Loss: 1415529.000, Test R²: 0.145\n",
      "Epoch: 033, Train Loss: 1431227.000, Train R²: 0.171, Test Loss: 1381406.125, Test R²: 0.165\n",
      "Epoch: 034, Train Loss: 1402325.250, Train R²: 0.188, Test Loss: 1351779.250, Test R²: 0.183\n",
      "Epoch: 035, Train Loss: 1378756.875, Train R²: 0.201, Test Loss: 1327236.250, Test R²: 0.198\n",
      "Epoch: 036, Train Loss: 1360694.625, Train R²: 0.212, Test Loss: 1307942.250, Test R²: 0.210\n",
      "Epoch: 037, Train Loss: 1347744.875, Train R²: 0.219, Test Loss: 1293483.625, Test R²: 0.218\n",
      "Epoch: 038, Train Loss: 1338941.125, Train R²: 0.224, Test Loss: 1282909.625, Test R²: 0.225\n",
      "Epoch: 039, Train Loss: 1332936.375, Train R²: 0.228, Test Loss: 1274942.625, Test R²: 0.230\n",
      "Epoch: 040, Train Loss: 1328299.750, Train R²: 0.231, Test Loss: 1268215.500, Test R²: 0.234\n",
      "Epoch: 041, Train Loss: 1323807.750, Train R²: 0.233, Test Loss: 1261557.375, Test R²: 0.238\n",
      "Epoch: 042, Train Loss: 1318483.875, Train R²: 0.236, Test Loss: 1254082.875, Test R²: 0.242\n",
      "Epoch: 043, Train Loss: 1311721.125, Train R²: 0.240, Test Loss: 1245269.625, Test R²: 0.248\n",
      "Epoch: 044, Train Loss: 1303311.000, Train R²: 0.245, Test Loss: 1234967.000, Test R²: 0.254\n",
      "Epoch: 045, Train Loss: 1293380.000, Train R²: 0.251, Test Loss: 1223313.375, Test R²: 0.261\n",
      "Epoch: 046, Train Loss: 1282252.500, Train R²: 0.257, Test Loss: 1210698.000, Test R²: 0.268\n",
      "Epoch: 047, Train Loss: 1270384.125, Train R²: 0.264, Test Loss: 1197592.000, Test R²: 0.276\n",
      "Epoch: 048, Train Loss: 1258264.750, Train R²: 0.271, Test Loss: 1184478.500, Test R²: 0.284\n",
      "Epoch: 049, Train Loss: 1246325.375, Train R²: 0.278, Test Loss: 1171760.500, Test R²: 0.292\n",
      "Epoch: 050, Train Loss: 1234906.250, Train R²: 0.285, Test Loss: 1159743.500, Test R²: 0.299\n",
      "Epoch: 051, Train Loss: 1224245.000, Train R²: 0.291, Test Loss: 1148607.125, Test R²: 0.306\n",
      "Epoch: 052, Train Loss: 1214476.750, Train R²: 0.296, Test Loss: 1138469.500, Test R²: 0.312\n",
      "Epoch: 053, Train Loss: 1205641.625, Train R²: 0.302, Test Loss: 1129332.375, Test R²: 0.318\n",
      "Epoch: 054, Train Loss: 1197706.875, Train R²: 0.306, Test Loss: 1121120.375, Test R²: 0.323\n",
      "Epoch: 055, Train Loss: 1190580.000, Train R²: 0.310, Test Loss: 1113705.000, Test R²: 0.327\n",
      "Epoch: 056, Train Loss: 1184191.875, Train R²: 0.314, Test Loss: 1107003.625, Test R²: 0.331\n",
      "Epoch: 057, Train Loss: 1178384.750, Train R²: 0.317, Test Loss: 1100763.500, Test R²: 0.335\n",
      "Epoch: 058, Train Loss: 1173012.125, Train R²: 0.320, Test Loss: 1094831.875, Test R²: 0.338\n",
      "Epoch: 059, Train Loss: 1167983.250, Train R²: 0.323, Test Loss: 1089085.000, Test R²: 0.342\n",
      "Epoch: 060, Train Loss: 1163278.250, Train R²: 0.326, Test Loss: 1083482.375, Test R²: 0.345\n",
      "Epoch: 061, Train Loss: 1158796.250, Train R²: 0.329, Test Loss: 1077974.375, Test R²: 0.349\n",
      "Epoch: 062, Train Loss: 1154489.750, Train R²: 0.331, Test Loss: 1072521.500, Test R²: 0.352\n",
      "Epoch: 063, Train Loss: 1150345.875, Train R²: 0.334, Test Loss: 1067065.750, Test R²: 0.355\n",
      "Epoch: 064, Train Loss: 1146386.625, Train R²: 0.336, Test Loss: 1061607.875, Test R²: 0.359\n",
      "Epoch: 065, Train Loss: 1142613.125, Train R²: 0.338, Test Loss: 1056184.500, Test R²: 0.362\n",
      "Epoch: 066, Train Loss: 1139053.250, Train R²: 0.340, Test Loss: 1050844.375, Test R²: 0.365\n",
      "Epoch: 067, Train Loss: 1135738.875, Train R²: 0.342, Test Loss: 1045621.938, Test R²: 0.368\n",
      "Epoch: 068, Train Loss: 1132677.000, Train R²: 0.344, Test Loss: 1040544.125, Test R²: 0.371\n",
      "Epoch: 069, Train Loss: 1129882.125, Train R²: 0.345, Test Loss: 1035651.750, Test R²: 0.374\n",
      "Epoch: 070, Train Loss: 1127386.125, Train R²: 0.347, Test Loss: 1031004.625, Test R²: 0.377\n",
      "Epoch: 071, Train Loss: 1125197.125, Train R²: 0.348, Test Loss: 1026631.438, Test R²: 0.380\n",
      "Epoch: 072, Train Loss: 1123301.375, Train R²: 0.349, Test Loss: 1022565.312, Test R²: 0.382\n",
      "Epoch: 073, Train Loss: 1121678.625, Train R²: 0.350, Test Loss: 1018815.000, Test R²: 0.384\n",
      "Epoch: 074, Train Loss: 1120301.875, Train R²: 0.351, Test Loss: 1015385.375, Test R²: 0.386\n",
      "Epoch: 075, Train Loss: 1119136.375, Train R²: 0.352, Test Loss: 1012293.375, Test R²: 0.388\n",
      "Epoch: 076, Train Loss: 1118153.000, Train R²: 0.352, Test Loss: 1009532.875, Test R²: 0.390\n",
      "Epoch: 077, Train Loss: 1117316.875, Train R²: 0.353, Test Loss: 1007093.562, Test R²: 0.391\n",
      "Epoch: 078, Train Loss: 1116596.625, Train R²: 0.353, Test Loss: 1004972.500, Test R²: 0.393\n",
      "Epoch: 079, Train Loss: 1115965.500, Train R²: 0.354, Test Loss: 1003170.438, Test R²: 0.394\n",
      "Epoch: 080, Train Loss: 1115405.875, Train R²: 0.354, Test Loss: 1001654.750, Test R²: 0.395\n",
      "Epoch: 081, Train Loss: 1114903.875, Train R²: 0.354, Test Loss: 1000399.500, Test R²: 0.396\n",
      "Epoch: 082, Train Loss: 1114448.625, Train R²: 0.354, Test Loss: 999392.938, Test R²: 0.396\n",
      "Epoch: 083, Train Loss: 1114028.750, Train R²: 0.355, Test Loss: 998602.688, Test R²: 0.397\n",
      "Epoch: 084, Train Loss: 1113648.125, Train R²: 0.355, Test Loss: 998007.562, Test R²: 0.397\n",
      "Epoch: 085, Train Loss: 1113303.250, Train R²: 0.355, Test Loss: 997599.500, Test R²: 0.397\n",
      "Epoch: 086, Train Loss: 1112991.250, Train R²: 0.355, Test Loss: 997340.438, Test R²: 0.397\n",
      "Epoch: 087, Train Loss: 1112707.375, Train R²: 0.355, Test Loss: 997208.500, Test R²: 0.397\n",
      "Epoch: 088, Train Loss: 1112435.375, Train R²: 0.356, Test Loss: 997179.500, Test R²: 0.397\n",
      "Epoch: 089, Train Loss: 1112161.125, Train R²: 0.356, Test Loss: 997234.375, Test R²: 0.397\n",
      "Epoch: 090, Train Loss: 1111878.250, Train R²: 0.356, Test Loss: 997368.438, Test R²: 0.397\n",
      "Epoch: 091, Train Loss: 1111582.000, Train R²: 0.356, Test Loss: 997512.250, Test R²: 0.397\n",
      "Epoch: 092, Train Loss: 1111274.375, Train R²: 0.356, Test Loss: 997645.812, Test R²: 0.397\n",
      "Epoch: 093, Train Loss: 1110950.500, Train R²: 0.356, Test Loss: 997775.438, Test R²: 0.397\n",
      "Epoch: 094, Train Loss: 1110609.625, Train R²: 0.357, Test Loss: 997872.438, Test R²: 0.397\n",
      "Epoch: 095, Train Loss: 1110250.375, Train R²: 0.357, Test Loss: 997926.250, Test R²: 0.397\n",
      "Epoch: 096, Train Loss: 1109873.750, Train R²: 0.357, Test Loss: 997951.875, Test R²: 0.397\n",
      "Epoch: 097, Train Loss: 1109491.125, Train R²: 0.357, Test Loss: 997938.875, Test R²: 0.397\n",
      "Epoch: 098, Train Loss: 1109107.000, Train R²: 0.357, Test Loss: 997884.562, Test R²: 0.397\n",
      "Epoch: 099, Train Loss: 1108719.125, Train R²: 0.358, Test Loss: 997812.562, Test R²: 0.397\n",
      "Epoch: 100, Train Loss: 1108336.500, Train R²: 0.358, Test Loss: 997745.562, Test R²: 0.397\n",
      "Epoch: 101, Train Loss: 1107960.375, Train R²: 0.358, Test Loss: 997674.438, Test R²: 0.397\n",
      "Epoch: 102, Train Loss: 1107591.500, Train R²: 0.358, Test Loss: 997606.250, Test R²: 0.397\n",
      "Epoch: 103, Train Loss: 1107233.875, Train R²: 0.359, Test Loss: 997555.438, Test R²: 0.397\n",
      "Epoch: 104, Train Loss: 1106886.875, Train R²: 0.359, Test Loss: 997523.938, Test R²: 0.397\n",
      "Epoch: 105, Train Loss: 1106547.250, Train R²: 0.359, Test Loss: 997512.250, Test R²: 0.397\n",
      "Epoch: 106, Train Loss: 1106212.875, Train R²: 0.359, Test Loss: 997518.188, Test R²: 0.397\n",
      "Epoch: 107, Train Loss: 1105879.125, Train R²: 0.359, Test Loss: 997538.812, Test R²: 0.397\n",
      "Epoch: 108, Train Loss: 1105544.125, Train R²: 0.360, Test Loss: 997573.125, Test R²: 0.397\n",
      "Epoch: 109, Train Loss: 1105207.125, Train R²: 0.360, Test Loss: 997615.500, Test R²: 0.397\n",
      "Epoch: 110, Train Loss: 1104869.875, Train R²: 0.360, Test Loss: 997667.000, Test R²: 0.397\n",
      "Epoch: 111, Train Loss: 1104531.375, Train R²: 0.360, Test Loss: 997733.062, Test R²: 0.397\n",
      "Epoch: 112, Train Loss: 1104190.875, Train R²: 0.360, Test Loss: 997792.312, Test R²: 0.397\n",
      "Epoch: 113, Train Loss: 1103847.750, Train R²: 0.361, Test Loss: 997834.312, Test R²: 0.397\n",
      "Epoch: 114, Train Loss: 1103504.250, Train R²: 0.361, Test Loss: 997855.688, Test R²: 0.397\n",
      "Epoch: 115, Train Loss: 1103156.875, Train R²: 0.361, Test Loss: 997852.188, Test R²: 0.397\n",
      "Epoch: 116, Train Loss: 1102807.750, Train R²: 0.361, Test Loss: 997824.438, Test R²: 0.397\n",
      "Epoch: 117, Train Loss: 1102456.500, Train R²: 0.361, Test Loss: 997756.750, Test R²: 0.397\n",
      "Epoch: 118, Train Loss: 1102100.125, Train R²: 0.362, Test Loss: 997650.812, Test R²: 0.397\n",
      "Epoch: 119, Train Loss: 1101733.750, Train R²: 0.362, Test Loss: 997500.375, Test R²: 0.397\n",
      "Epoch: 120, Train Loss: 1101363.750, Train R²: 0.362, Test Loss: 997304.250, Test R²: 0.397\n",
      "Epoch: 121, Train Loss: 1100987.250, Train R²: 0.362, Test Loss: 997067.375, Test R²: 0.398\n",
      "Epoch: 122, Train Loss: 1100604.750, Train R²: 0.362, Test Loss: 996789.125, Test R²: 0.398\n",
      "Epoch: 123, Train Loss: 1100219.875, Train R²: 0.363, Test Loss: 996473.375, Test R²: 0.398\n",
      "Epoch: 124, Train Loss: 1099831.000, Train R²: 0.363, Test Loss: 996119.875, Test R²: 0.398\n",
      "Epoch: 125, Train Loss: 1099435.875, Train R²: 0.363, Test Loss: 995726.875, Test R²: 0.398\n",
      "Epoch: 126, Train Loss: 1099029.125, Train R²: 0.363, Test Loss: 995294.438, Test R²: 0.399\n",
      "Epoch: 127, Train Loss: 1098615.875, Train R²: 0.364, Test Loss: 994830.812, Test R²: 0.399\n",
      "Epoch: 128, Train Loss: 1098195.000, Train R²: 0.364, Test Loss: 994356.062, Test R²: 0.399\n",
      "Epoch: 129, Train Loss: 1097771.750, Train R²: 0.364, Test Loss: 993880.500, Test R²: 0.399\n",
      "Epoch: 130, Train Loss: 1097354.625, Train R²: 0.364, Test Loss: 993424.438, Test R²: 0.400\n",
      "Epoch: 131, Train Loss: 1096929.250, Train R²: 0.365, Test Loss: 993001.312, Test R²: 0.400\n",
      "Epoch: 132, Train Loss: 1096489.500, Train R²: 0.365, Test Loss: 992593.688, Test R²: 0.400\n",
      "Epoch: 133, Train Loss: 1096036.875, Train R²: 0.365, Test Loss: 992196.688, Test R²: 0.400\n",
      "Epoch: 134, Train Loss: 1095571.250, Train R²: 0.365, Test Loss: 991826.562, Test R²: 0.401\n",
      "Epoch: 135, Train Loss: 1095095.500, Train R²: 0.366, Test Loss: 991470.562, Test R²: 0.401\n",
      "Epoch: 136, Train Loss: 1094609.250, Train R²: 0.366, Test Loss: 991118.562, Test R²: 0.401\n",
      "Epoch: 137, Train Loss: 1094118.000, Train R²: 0.366, Test Loss: 990787.562, Test R²: 0.401\n",
      "Epoch: 138, Train Loss: 1093621.625, Train R²: 0.366, Test Loss: 990457.000, Test R²: 0.402\n",
      "Epoch: 139, Train Loss: 1093118.125, Train R²: 0.367, Test Loss: 990117.312, Test R²: 0.402\n",
      "Epoch: 140, Train Loss: 1092604.750, Train R²: 0.367, Test Loss: 989771.125, Test R²: 0.402\n",
      "Epoch: 141, Train Loss: 1092081.750, Train R²: 0.367, Test Loss: 989442.312, Test R²: 0.402\n",
      "Epoch: 142, Train Loss: 1091556.875, Train R²: 0.368, Test Loss: 989178.438, Test R²: 0.402\n",
      "Epoch: 143, Train Loss: 1091029.875, Train R²: 0.368, Test Loss: 988890.625, Test R²: 0.402\n",
      "Epoch: 144, Train Loss: 1090499.250, Train R²: 0.368, Test Loss: 988579.000, Test R²: 0.403\n",
      "Epoch: 145, Train Loss: 1089962.500, Train R²: 0.369, Test Loss: 988231.625, Test R²: 0.403\n",
      "Epoch: 146, Train Loss: 1089421.000, Train R²: 0.369, Test Loss: 987878.688, Test R²: 0.403\n",
      "Epoch: 147, Train Loss: 1088865.500, Train R²: 0.369, Test Loss: 987517.688, Test R²: 0.403\n",
      "Epoch: 148, Train Loss: 1088299.125, Train R²: 0.370, Test Loss: 987164.062, Test R²: 0.404\n",
      "Epoch: 149, Train Loss: 1087725.000, Train R²: 0.370, Test Loss: 986812.688, Test R²: 0.404\n",
      "Epoch: 150, Train Loss: 1087145.250, Train R²: 0.370, Test Loss: 986449.500, Test R²: 0.404\n",
      "Epoch: 151, Train Loss: 1086560.375, Train R²: 0.371, Test Loss: 986092.312, Test R²: 0.404\n",
      "Epoch: 152, Train Loss: 1085969.250, Train R²: 0.371, Test Loss: 985751.938, Test R²: 0.404\n",
      "Epoch: 153, Train Loss: 1085373.625, Train R²: 0.371, Test Loss: 985427.375, Test R²: 0.405\n",
      "Epoch: 154, Train Loss: 1084767.250, Train R²: 0.372, Test Loss: 985116.562, Test R²: 0.405\n",
      "Epoch: 155, Train Loss: 1084155.000, Train R²: 0.372, Test Loss: 984802.938, Test R²: 0.405\n",
      "Epoch: 156, Train Loss: 1083538.500, Train R²: 0.372, Test Loss: 984486.938, Test R²: 0.405\n",
      "Epoch: 157, Train Loss: 1082919.000, Train R²: 0.373, Test Loss: 984162.750, Test R²: 0.405\n",
      "Epoch: 158, Train Loss: 1082296.000, Train R²: 0.373, Test Loss: 983828.250, Test R²: 0.406\n",
      "Epoch: 159, Train Loss: 1081671.375, Train R²: 0.373, Test Loss: 983485.562, Test R²: 0.406\n",
      "Epoch: 160, Train Loss: 1081044.125, Train R²: 0.374, Test Loss: 983140.000, Test R²: 0.406\n",
      "Epoch: 161, Train Loss: 1080418.625, Train R²: 0.374, Test Loss: 982796.938, Test R²: 0.406\n",
      "Epoch: 162, Train Loss: 1079788.750, Train R²: 0.374, Test Loss: 982446.750, Test R²: 0.406\n",
      "Epoch: 163, Train Loss: 1079156.000, Train R²: 0.375, Test Loss: 982094.812, Test R²: 0.407\n",
      "Epoch: 164, Train Loss: 1078519.875, Train R²: 0.375, Test Loss: 981739.062, Test R²: 0.407\n",
      "Epoch: 165, Train Loss: 1077882.125, Train R²: 0.376, Test Loss: 981398.812, Test R²: 0.407\n",
      "Epoch: 166, Train Loss: 1077243.375, Train R²: 0.376, Test Loss: 981079.625, Test R²: 0.407\n",
      "Epoch: 167, Train Loss: 1076598.500, Train R²: 0.376, Test Loss: 980778.688, Test R²: 0.407\n",
      "Epoch: 168, Train Loss: 1075949.000, Train R²: 0.377, Test Loss: 980492.562, Test R²: 0.408\n",
      "Epoch: 169, Train Loss: 1075301.375, Train R²: 0.377, Test Loss: 980214.688, Test R²: 0.408\n",
      "Epoch: 170, Train Loss: 1074650.250, Train R²: 0.377, Test Loss: 979928.250, Test R²: 0.408\n",
      "Epoch: 171, Train Loss: 1074000.375, Train R²: 0.378, Test Loss: 979639.750, Test R²: 0.408\n",
      "Epoch: 172, Train Loss: 1073346.500, Train R²: 0.378, Test Loss: 979355.000, Test R²: 0.408\n",
      "Epoch: 173, Train Loss: 1072688.250, Train R²: 0.379, Test Loss: 979076.125, Test R²: 0.408\n",
      "Epoch: 174, Train Loss: 1072025.625, Train R²: 0.379, Test Loss: 978799.812, Test R²: 0.409\n",
      "Epoch: 175, Train Loss: 1071356.375, Train R²: 0.379, Test Loss: 978529.750, Test R²: 0.409\n",
      "Epoch: 176, Train Loss: 1070681.625, Train R²: 0.380, Test Loss: 978264.938, Test R²: 0.409\n",
      "Epoch: 177, Train Loss: 1070007.000, Train R²: 0.380, Test Loss: 978006.000, Test R²: 0.409\n",
      "Epoch: 178, Train Loss: 1069328.625, Train R²: 0.381, Test Loss: 977780.500, Test R²: 0.409\n",
      "Epoch: 179, Train Loss: 1068643.750, Train R²: 0.381, Test Loss: 977570.188, Test R²: 0.409\n",
      "Epoch: 180, Train Loss: 1067953.875, Train R²: 0.381, Test Loss: 977363.938, Test R²: 0.409\n",
      "Epoch: 181, Train Loss: 1067259.875, Train R²: 0.382, Test Loss: 977157.125, Test R²: 0.410\n",
      "Epoch: 182, Train Loss: 1066562.375, Train R²: 0.382, Test Loss: 976955.562, Test R²: 0.410\n",
      "Epoch: 183, Train Loss: 1065861.875, Train R²: 0.383, Test Loss: 976765.125, Test R²: 0.410\n",
      "Epoch: 184, Train Loss: 1065158.625, Train R²: 0.383, Test Loss: 976594.188, Test R²: 0.410\n",
      "Epoch: 185, Train Loss: 1064453.250, Train R²: 0.383, Test Loss: 976445.562, Test R²: 0.410\n",
      "Epoch: 186, Train Loss: 1063744.625, Train R²: 0.384, Test Loss: 976318.125, Test R²: 0.410\n",
      "Epoch: 187, Train Loss: 1063032.500, Train R²: 0.384, Test Loss: 976198.375, Test R²: 0.410\n",
      "Epoch: 188, Train Loss: 1062317.875, Train R²: 0.385, Test Loss: 976081.875, Test R²: 0.410\n",
      "Epoch: 189, Train Loss: 1061602.875, Train R²: 0.385, Test Loss: 975971.000, Test R²: 0.410\n",
      "Epoch: 190, Train Loss: 1060884.250, Train R²: 0.385, Test Loss: 975871.875, Test R²: 0.410\n",
      "Epoch: 191, Train Loss: 1060160.125, Train R²: 0.386, Test Loss: 975791.188, Test R²: 0.410\n",
      "Epoch: 192, Train Loss: 1059433.500, Train R²: 0.386, Test Loss: 975725.062, Test R²: 0.410\n",
      "Epoch: 193, Train Loss: 1058703.375, Train R²: 0.387, Test Loss: 975656.500, Test R²: 0.410\n",
      "Epoch: 194, Train Loss: 1057961.250, Train R²: 0.387, Test Loss: 975577.312, Test R²: 0.411\n",
      "Epoch: 195, Train Loss: 1057212.750, Train R²: 0.388, Test Loss: 975488.500, Test R²: 0.411\n",
      "Epoch: 196, Train Loss: 1056472.125, Train R²: 0.388, Test Loss: 975433.062, Test R²: 0.411\n",
      "Epoch: 197, Train Loss: 1055729.750, Train R²: 0.388, Test Loss: 975300.688, Test R²: 0.411\n",
      "Epoch: 198, Train Loss: 1054972.750, Train R²: 0.389, Test Loss: 975062.062, Test R²: 0.411\n",
      "Epoch: 199, Train Loss: 1054211.000, Train R²: 0.389, Test Loss: 974768.438, Test R²: 0.411\n",
      "Epoch: 200, Train Loss: 1053446.125, Train R²: 0.390, Test Loss: 974462.938, Test R²: 0.411\n",
      "Epoch: 201, Train Loss: 1052679.500, Train R²: 0.390, Test Loss: 974177.562, Test R²: 0.411\n",
      "Epoch: 202, Train Loss: 1051904.375, Train R²: 0.391, Test Loss: 973941.750, Test R²: 0.412\n",
      "Epoch: 203, Train Loss: 1051122.375, Train R²: 0.391, Test Loss: 973722.375, Test R²: 0.412\n",
      "Epoch: 204, Train Loss: 1050343.875, Train R²: 0.392, Test Loss: 973515.125, Test R²: 0.412\n",
      "Epoch: 205, Train Loss: 1049569.000, Train R²: 0.392, Test Loss: 973374.438, Test R²: 0.412\n",
      "Epoch: 206, Train Loss: 1048792.375, Train R²: 0.392, Test Loss: 973267.562, Test R²: 0.412\n",
      "Epoch: 207, Train Loss: 1048009.500, Train R²: 0.393, Test Loss: 973153.000, Test R²: 0.412\n",
      "Epoch: 208, Train Loss: 1047219.000, Train R²: 0.393, Test Loss: 973007.688, Test R²: 0.412\n",
      "Epoch: 209, Train Loss: 1046423.625, Train R²: 0.394, Test Loss: 972829.250, Test R²: 0.412\n",
      "Epoch: 210, Train Loss: 1045636.375, Train R²: 0.394, Test Loss: 972639.875, Test R²: 0.412\n",
      "Epoch: 211, Train Loss: 1044849.688, Train R²: 0.395, Test Loss: 972459.812, Test R²: 0.412\n",
      "Epoch: 212, Train Loss: 1044061.688, Train R²: 0.395, Test Loss: 972295.000, Test R²: 0.413\n",
      "Epoch: 213, Train Loss: 1043268.562, Train R²: 0.396, Test Loss: 972153.000, Test R²: 0.413\n",
      "Epoch: 214, Train Loss: 1042469.375, Train R²: 0.396, Test Loss: 972055.125, Test R²: 0.413\n",
      "Epoch: 215, Train Loss: 1041666.625, Train R²: 0.397, Test Loss: 971972.562, Test R²: 0.413\n",
      "Epoch: 216, Train Loss: 1040862.688, Train R²: 0.397, Test Loss: 971871.500, Test R²: 0.413\n",
      "Epoch: 217, Train Loss: 1040056.375, Train R²: 0.397, Test Loss: 971751.188, Test R²: 0.413\n",
      "Epoch: 218, Train Loss: 1039246.875, Train R²: 0.398, Test Loss: 971647.812, Test R²: 0.413\n",
      "Epoch: 219, Train Loss: 1038430.062, Train R²: 0.398, Test Loss: 971585.875, Test R²: 0.413\n",
      "Epoch: 220, Train Loss: 1037603.875, Train R²: 0.399, Test Loss: 971559.312, Test R²: 0.413\n",
      "Epoch: 221, Train Loss: 1036776.062, Train R²: 0.399, Test Loss: 971544.938, Test R²: 0.413\n",
      "Epoch: 222, Train Loss: 1035947.438, Train R²: 0.400, Test Loss: 971487.688, Test R²: 0.413\n",
      "Epoch: 223, Train Loss: 1035114.375, Train R²: 0.400, Test Loss: 971396.312, Test R²: 0.413\n",
      "Epoch: 224, Train Loss: 1034280.812, Train R²: 0.401, Test Loss: 971354.625, Test R²: 0.413\n",
      "Epoch: 225, Train Loss: 1033449.062, Train R²: 0.401, Test Loss: 971366.688, Test R²: 0.413\n",
      "Epoch: 226, Train Loss: 1032619.250, Train R²: 0.402, Test Loss: 971389.938, Test R²: 0.413\n",
      "Epoch: 227, Train Loss: 1031785.438, Train R²: 0.402, Test Loss: 971365.938, Test R²: 0.413\n",
      "Epoch: 228, Train Loss: 1030947.938, Train R²: 0.403, Test Loss: 971288.312, Test R²: 0.413\n",
      "Epoch: 229, Train Loss: 1030113.312, Train R²: 0.403, Test Loss: 971245.875, Test R²: 0.413\n",
      "Epoch: 230, Train Loss: 1029280.062, Train R²: 0.404, Test Loss: 971247.625, Test R²: 0.413\n",
      "Epoch: 231, Train Loss: 1028448.438, Train R²: 0.404, Test Loss: 971304.188, Test R²: 0.413\n",
      "Epoch: 232, Train Loss: 1027615.312, Train R²: 0.405, Test Loss: 971406.562, Test R²: 0.413\n",
      "Epoch: 233, Train Loss: 1026764.750, Train R²: 0.405, Test Loss: 971563.625, Test R²: 0.413\n",
      "Epoch: 234, Train Loss: 1025892.312, Train R²: 0.406, Test Loss: 971866.750, Test R²: 0.413\n",
      "Epoch: 235, Train Loss: 1025001.188, Train R²: 0.406, Test Loss: 972347.750, Test R²: 0.412\n",
      "Epoch: 236, Train Loss: 1024118.375, Train R²: 0.407, Test Loss: 972807.375, Test R²: 0.412\n",
      "Epoch: 237, Train Loss: 1023242.375, Train R²: 0.407, Test Loss: 973079.875, Test R²: 0.412\n",
      "Epoch: 238, Train Loss: 1022361.500, Train R²: 0.408, Test Loss: 973100.812, Test R²: 0.412\n",
      "Epoch: 239, Train Loss: 1021482.750, Train R²: 0.408, Test Loss: 972952.250, Test R²: 0.412\n",
      "Epoch: 240, Train Loss: 1020605.562, Train R²: 0.409, Test Loss: 972812.188, Test R²: 0.412\n",
      "Epoch: 241, Train Loss: 1019735.312, Train R²: 0.409, Test Loss: 972843.875, Test R²: 0.412\n",
      "Epoch: 242, Train Loss: 1018863.938, Train R²: 0.410, Test Loss: 973097.938, Test R²: 0.412\n",
      "Epoch: 243, Train Loss: 1017989.750, Train R²: 0.410, Test Loss: 973496.438, Test R²: 0.412\n",
      "Epoch: 244, Train Loss: 1017123.688, Train R²: 0.411, Test Loss: 973968.062, Test R²: 0.412\n",
      "Epoch: 245, Train Loss: 1016265.250, Train R²: 0.411, Test Loss: 974458.812, Test R²: 0.411\n",
      "Epoch: 246, Train Loss: 1015408.750, Train R²: 0.412, Test Loss: 974896.438, Test R²: 0.411\n",
      "Epoch: 247, Train Loss: 1014543.375, Train R²: 0.412, Test Loss: 975205.125, Test R²: 0.411\n",
      "Epoch: 248, Train Loss: 1013678.375, Train R²: 0.413, Test Loss: 975457.312, Test R²: 0.411\n",
      "Epoch: 249, Train Loss: 1012813.500, Train R²: 0.413, Test Loss: 975695.250, Test R²: 0.410\n",
      "Epoch: 250, Train Loss: 1011944.375, Train R²: 0.414, Test Loss: 975906.125, Test R²: 0.410\n",
      "Epoch: 251, Train Loss: 1011080.375, Train R²: 0.414, Test Loss: 976142.188, Test R²: 0.410\n",
      "Epoch: 252, Train Loss: 1010222.562, Train R²: 0.415, Test Loss: 976447.562, Test R²: 0.410\n",
      "Epoch: 253, Train Loss: 1009360.750, Train R²: 0.415, Test Loss: 976813.562, Test R²: 0.410\n",
      "Epoch: 254, Train Loss: 1008490.562, Train R²: 0.416, Test Loss: 977227.688, Test R²: 0.410\n",
      "Epoch: 255, Train Loss: 1007628.625, Train R²: 0.416, Test Loss: 977648.938, Test R²: 0.409\n",
      "Epoch: 256, Train Loss: 1006773.000, Train R²: 0.417, Test Loss: 977995.188, Test R²: 0.409\n",
      "Epoch: 257, Train Loss: 1005917.312, Train R²: 0.417, Test Loss: 978245.375, Test R²: 0.409\n",
      "Epoch: 258, Train Loss: 1005062.562, Train R²: 0.418, Test Loss: 978545.375, Test R²: 0.409\n",
      "Epoch: 259, Train Loss: 1004205.000, Train R²: 0.418, Test Loss: 978975.438, Test R²: 0.408\n",
      "Epoch: 260, Train Loss: 1003346.312, Train R²: 0.419, Test Loss: 979455.125, Test R²: 0.408\n",
      "Epoch: 261, Train Loss: 1002484.062, Train R²: 0.419, Test Loss: 979901.188, Test R²: 0.408\n",
      "Epoch: 262, Train Loss: 1001625.062, Train R²: 0.420, Test Loss: 980254.438, Test R²: 0.408\n",
      "Epoch: 263, Train Loss: 1000768.062, Train R²: 0.420, Test Loss: 980536.562, Test R²: 0.408\n",
      "Epoch: 264, Train Loss: 999907.312, Train R²: 0.421, Test Loss: 980832.312, Test R²: 0.407\n",
      "Epoch: 265, Train Loss: 999036.500, Train R²: 0.421, Test Loss: 981213.375, Test R²: 0.407\n",
      "Epoch: 266, Train Loss: 998170.062, Train R²: 0.422, Test Loss: 981612.500, Test R²: 0.407\n",
      "Epoch: 267, Train Loss: 997282.688, Train R²: 0.422, Test Loss: 981967.500, Test R²: 0.407\n",
      "Epoch: 268, Train Loss: 996357.750, Train R²: 0.423, Test Loss: 982330.062, Test R²: 0.406\n",
      "Epoch: 269, Train Loss: 995415.188, Train R²: 0.423, Test Loss: 982579.500, Test R²: 0.406\n",
      "Epoch: 270, Train Loss: 994510.938, Train R²: 0.424, Test Loss: 982943.062, Test R²: 0.406\n",
      "Epoch: 271, Train Loss: 993597.062, Train R²: 0.424, Test Loss: 983604.438, Test R²: 0.406\n",
      "Epoch: 272, Train Loss: 992695.250, Train R²: 0.425, Test Loss: 984173.312, Test R²: 0.405\n",
      "Epoch: 273, Train Loss: 991764.188, Train R²: 0.425, Test Loss: 984429.062, Test R²: 0.405\n",
      "Epoch: 274, Train Loss: 990810.000, Train R²: 0.426, Test Loss: 984502.938, Test R²: 0.405\n",
      "Epoch: 275, Train Loss: 989859.125, Train R²: 0.427, Test Loss: 984557.750, Test R²: 0.405\n",
      "Epoch: 276, Train Loss: 988909.688, Train R²: 0.427, Test Loss: 984843.688, Test R²: 0.405\n",
      "Epoch: 277, Train Loss: 987946.438, Train R²: 0.428, Test Loss: 985161.688, Test R²: 0.405\n",
      "Epoch: 278, Train Loss: 986988.938, Train R²: 0.428, Test Loss: 985353.312, Test R²: 0.405\n",
      "Epoch: 279, Train Loss: 986027.250, Train R²: 0.429, Test Loss: 985373.250, Test R²: 0.405\n",
      "Epoch: 280, Train Loss: 985052.000, Train R²: 0.429, Test Loss: 985373.062, Test R²: 0.405\n",
      "Epoch: 281, Train Loss: 984123.000, Train R²: 0.430, Test Loss: 985709.688, Test R²: 0.404\n",
      "Epoch: 282, Train Loss: 983182.062, Train R²: 0.430, Test Loss: 986006.188, Test R²: 0.404\n",
      "Epoch: 283, Train Loss: 982230.000, Train R²: 0.431, Test Loss: 986120.062, Test R²: 0.404\n",
      "Epoch: 284, Train Loss: 981262.062, Train R²: 0.432, Test Loss: 985940.562, Test R²: 0.404\n",
      "Epoch: 285, Train Loss: 980293.250, Train R²: 0.432, Test Loss: 985812.750, Test R²: 0.404\n",
      "Epoch: 286, Train Loss: 979300.938, Train R²: 0.433, Test Loss: 985988.500, Test R²: 0.404\n",
      "Epoch: 287, Train Loss: 978312.188, Train R²: 0.433, Test Loss: 986373.562, Test R²: 0.404\n",
      "Epoch: 288, Train Loss: 977320.375, Train R²: 0.434, Test Loss: 986682.812, Test R²: 0.404\n",
      "Epoch: 289, Train Loss: 976320.438, Train R²: 0.434, Test Loss: 986896.312, Test R²: 0.404\n",
      "Epoch: 290, Train Loss: 975325.812, Train R²: 0.435, Test Loss: 987201.875, Test R²: 0.404\n",
      "Epoch: 291, Train Loss: 974344.000, Train R²: 0.436, Test Loss: 987363.438, Test R²: 0.403\n",
      "Epoch: 292, Train Loss: 973351.375, Train R²: 0.436, Test Loss: 987826.688, Test R²: 0.403\n",
      "Epoch: 293, Train Loss: 972337.750, Train R²: 0.437, Test Loss: 988399.625, Test R²: 0.403\n",
      "Epoch: 294, Train Loss: 971309.500, Train R²: 0.437, Test Loss: 988601.438, Test R²: 0.403\n",
      "Epoch: 295, Train Loss: 970272.125, Train R²: 0.438, Test Loss: 988634.000, Test R²: 0.403\n",
      "Epoch: 296, Train Loss: 969221.312, Train R²: 0.439, Test Loss: 988902.562, Test R²: 0.402\n",
      "Epoch: 297, Train Loss: 968163.562, Train R²: 0.439, Test Loss: 988889.375, Test R²: 0.402\n",
      "Epoch: 298, Train Loss: 967118.438, Train R²: 0.440, Test Loss: 988756.938, Test R²: 0.403\n",
      "Epoch: 299, Train Loss: 966060.000, Train R²: 0.440, Test Loss: 989372.875, Test R²: 0.402\n",
      "Epoch: 300, Train Loss: 964952.312, Train R²: 0.441, Test Loss: 989667.250, Test R²: 0.402\n",
      "Epoch: 301, Train Loss: 963882.000, Train R²: 0.442, Test Loss: 990340.938, Test R²: 0.402\n",
      "Epoch: 302, Train Loss: 962833.000, Train R²: 0.442, Test Loss: 991225.688, Test R²: 0.401\n",
      "Epoch: 303, Train Loss: 961738.938, Train R²: 0.443, Test Loss: 991636.125, Test R²: 0.401\n",
      "Epoch: 304, Train Loss: 960616.875, Train R²: 0.444, Test Loss: 992663.875, Test R²: 0.400\n",
      "Epoch: 305, Train Loss: 959526.312, Train R²: 0.444, Test Loss: 993612.188, Test R²: 0.400\n",
      "Epoch: 306, Train Loss: 958429.938, Train R²: 0.445, Test Loss: 994215.312, Test R²: 0.399\n",
      "Epoch: 307, Train Loss: 957288.625, Train R²: 0.445, Test Loss: 995067.500, Test R²: 0.399\n",
      "Epoch: 308, Train Loss: 956140.312, Train R²: 0.446, Test Loss: 995512.625, Test R²: 0.398\n",
      "Epoch: 309, Train Loss: 954984.062, Train R²: 0.447, Test Loss: 995611.812, Test R²: 0.398\n",
      "Epoch: 310, Train Loss: 953775.625, Train R²: 0.447, Test Loss: 995925.438, Test R²: 0.398\n",
      "Epoch: 311, Train Loss: 952570.750, Train R²: 0.448, Test Loss: 996121.062, Test R²: 0.398\n",
      "Epoch: 312, Train Loss: 951393.250, Train R²: 0.449, Test Loss: 996726.312, Test R²: 0.398\n",
      "Epoch: 313, Train Loss: 950181.812, Train R²: 0.450, Test Loss: 997469.062, Test R²: 0.397\n",
      "Epoch: 314, Train Loss: 948934.375, Train R²: 0.450, Test Loss: 997475.375, Test R²: 0.397\n",
      "Epoch: 315, Train Loss: 947710.250, Train R²: 0.451, Test Loss: 997806.562, Test R²: 0.397\n",
      "Epoch: 316, Train Loss: 946584.500, Train R²: 0.452, Test Loss: 997882.938, Test R²: 0.397\n",
      "Epoch: 317, Train Loss: 945474.500, Train R²: 0.452, Test Loss: 998273.188, Test R²: 0.397\n",
      "Epoch: 318, Train Loss: 944362.250, Train R²: 0.453, Test Loss: 998687.062, Test R²: 0.397\n",
      "Epoch: 319, Train Loss: 943232.062, Train R²: 0.454, Test Loss: 998530.750, Test R²: 0.397\n",
      "Epoch: 320, Train Loss: 942087.875, Train R²: 0.454, Test Loss: 998710.750, Test R²: 0.397\n",
      "Epoch: 321, Train Loss: 940948.750, Train R²: 0.455, Test Loss: 998689.375, Test R²: 0.397\n",
      "Epoch: 322, Train Loss: 939798.625, Train R²: 0.456, Test Loss: 999329.188, Test R²: 0.396\n",
      "Epoch: 323, Train Loss: 938662.188, Train R²: 0.456, Test Loss: 1000054.812, Test R²: 0.396\n",
      "Epoch: 324, Train Loss: 937521.312, Train R²: 0.457, Test Loss: 1000887.750, Test R²: 0.395\n",
      "Epoch: 325, Train Loss: 936371.438, Train R²: 0.458, Test Loss: 1001810.938, Test R²: 0.395\n",
      "Epoch: 326, Train Loss: 935220.750, Train R²: 0.458, Test Loss: 1002190.812, Test R²: 0.394\n",
      "Epoch: 327, Train Loss: 934068.250, Train R²: 0.459, Test Loss: 1003049.125, Test R²: 0.394\n",
      "Epoch: 328, Train Loss: 932972.375, Train R²: 0.460, Test Loss: 1003218.938, Test R²: 0.394\n",
      "Epoch: 329, Train Loss: 931840.500, Train R²: 0.460, Test Loss: 1004402.188, Test R²: 0.393\n",
      "Epoch: 330, Train Loss: 930684.938, Train R²: 0.461, Test Loss: 1004516.500, Test R²: 0.393\n",
      "Epoch: 331, Train Loss: 929552.562, Train R²: 0.462, Test Loss: 1005039.625, Test R²: 0.393\n",
      "Epoch: 332, Train Loss: 928476.500, Train R²: 0.462, Test Loss: 1005656.500, Test R²: 0.392\n",
      "Epoch: 333, Train Loss: 927415.125, Train R²: 0.463, Test Loss: 1005904.062, Test R²: 0.392\n",
      "Epoch: 334, Train Loss: 926341.312, Train R²: 0.463, Test Loss: 1007072.500, Test R²: 0.392\n",
      "Epoch: 335, Train Loss: 925212.312, Train R²: 0.464, Test Loss: 1007054.188, Test R²: 0.392\n",
      "Epoch: 336, Train Loss: 924061.312, Train R²: 0.465, Test Loss: 1007988.875, Test R²: 0.391\n",
      "Epoch: 337, Train Loss: 922949.062, Train R²: 0.465, Test Loss: 1008741.875, Test R²: 0.390\n",
      "Epoch: 338, Train Loss: 921904.000, Train R²: 0.466, Test Loss: 1009640.812, Test R²: 0.390\n",
      "Epoch: 339, Train Loss: 920850.625, Train R²: 0.467, Test Loss: 1010845.250, Test R²: 0.389\n",
      "Epoch: 340, Train Loss: 919779.688, Train R²: 0.467, Test Loss: 1010731.500, Test R²: 0.389\n",
      "Epoch: 341, Train Loss: 918665.625, Train R²: 0.468, Test Loss: 1011866.812, Test R²: 0.389\n",
      "Epoch: 342, Train Loss: 917544.312, Train R²: 0.468, Test Loss: 1012022.938, Test R²: 0.389\n",
      "Epoch: 343, Train Loss: 916388.312, Train R²: 0.469, Test Loss: 1012812.375, Test R²: 0.388\n",
      "Epoch: 344, Train Loss: 915277.125, Train R²: 0.470, Test Loss: 1013673.812, Test R²: 0.388\n",
      "Epoch: 345, Train Loss: 914194.312, Train R²: 0.470, Test Loss: 1014202.000, Test R²: 0.387\n",
      "Epoch: 346, Train Loss: 913089.500, Train R²: 0.471, Test Loss: 1015749.125, Test R²: 0.386\n",
      "Epoch: 347, Train Loss: 912004.938, Train R²: 0.472, Test Loss: 1016025.188, Test R²: 0.386\n",
      "Epoch: 348, Train Loss: 910922.125, Train R²: 0.472, Test Loss: 1017818.375, Test R²: 0.385\n",
      "Epoch: 349, Train Loss: 909867.500, Train R²: 0.473, Test Loss: 1017576.500, Test R²: 0.385\n",
      "Epoch: 350, Train Loss: 908788.438, Train R²: 0.474, Test Loss: 1019225.375, Test R²: 0.384\n",
      "Epoch: 351, Train Loss: 907644.625, Train R²: 0.474, Test Loss: 1018285.562, Test R²: 0.385\n",
      "Epoch: 352, Train Loss: 906476.312, Train R²: 0.475, Test Loss: 1019626.188, Test R²: 0.384\n",
      "Epoch: 353, Train Loss: 905329.312, Train R²: 0.476, Test Loss: 1019991.438, Test R²: 0.384\n",
      "Epoch: 354, Train Loss: 904220.562, Train R²: 0.476, Test Loss: 1020358.375, Test R²: 0.383\n",
      "Epoch: 355, Train Loss: 903148.000, Train R²: 0.477, Test Loss: 1021482.438, Test R²: 0.383\n",
      "Epoch: 356, Train Loss: 902148.312, Train R²: 0.477, Test Loss: 1020543.125, Test R²: 0.383\n",
      "Epoch: 357, Train Loss: 901123.312, Train R²: 0.478, Test Loss: 1022773.500, Test R²: 0.382\n",
      "Epoch: 358, Train Loss: 899903.688, Train R²: 0.479, Test Loss: 1021073.750, Test R²: 0.383\n",
      "Epoch: 359, Train Loss: 898624.875, Train R²: 0.479, Test Loss: 1022733.625, Test R²: 0.382\n",
      "Epoch: 360, Train Loss: 897528.562, Train R²: 0.480, Test Loss: 1023782.312, Test R²: 0.381\n",
      "Epoch: 361, Train Loss: 896536.812, Train R²: 0.481, Test Loss: 1023430.438, Test R²: 0.382\n",
      "Epoch: 362, Train Loss: 895427.438, Train R²: 0.481, Test Loss: 1025636.062, Test R²: 0.380\n",
      "Epoch: 363, Train Loss: 894162.312, Train R²: 0.482, Test Loss: 1024027.125, Test R²: 0.381\n",
      "Epoch: 364, Train Loss: 892845.125, Train R²: 0.483, Test Loss: 1025402.625, Test R²: 0.380\n",
      "Epoch: 365, Train Loss: 891682.375, Train R²: 0.483, Test Loss: 1025955.188, Test R²: 0.380\n",
      "Epoch: 366, Train Loss: 890649.375, Train R²: 0.484, Test Loss: 1025545.938, Test R²: 0.380\n",
      "Epoch: 367, Train Loss: 889585.875, Train R²: 0.485, Test Loss: 1028298.500, Test R²: 0.379\n",
      "Epoch: 368, Train Loss: 888333.938, Train R²: 0.485, Test Loss: 1026942.938, Test R²: 0.380\n",
      "Epoch: 369, Train Loss: 886998.250, Train R²: 0.486, Test Loss: 1029086.438, Test R²: 0.378\n",
      "Epoch: 370, Train Loss: 885758.375, Train R²: 0.487, Test Loss: 1029807.000, Test R²: 0.378\n",
      "Epoch: 371, Train Loss: 884590.875, Train R²: 0.488, Test Loss: 1030103.750, Test R²: 0.378\n",
      "Epoch: 372, Train Loss: 883488.812, Train R²: 0.488, Test Loss: 1032603.812, Test R²: 0.376\n",
      "Epoch: 373, Train Loss: 882340.625, Train R²: 0.489, Test Loss: 1031308.375, Test R²: 0.377\n",
      "Epoch: 374, Train Loss: 881043.812, Train R²: 0.490, Test Loss: 1034078.250, Test R²: 0.375\n",
      "Epoch: 375, Train Loss: 879736.750, Train R²: 0.490, Test Loss: 1033618.812, Test R²: 0.375\n",
      "Epoch: 376, Train Loss: 878511.188, Train R²: 0.491, Test Loss: 1035015.688, Test R²: 0.375\n",
      "Epoch: 377, Train Loss: 877360.062, Train R²: 0.492, Test Loss: 1037063.000, Test R²: 0.373\n",
      "Epoch: 378, Train Loss: 876254.938, Train R²: 0.492, Test Loss: 1036511.562, Test R²: 0.374\n",
      "Epoch: 379, Train Loss: 875100.312, Train R²: 0.493, Test Loss: 1039409.875, Test R²: 0.372\n",
      "Epoch: 380, Train Loss: 873801.562, Train R²: 0.494, Test Loss: 1037682.000, Test R²: 0.373\n",
      "Epoch: 381, Train Loss: 872416.688, Train R²: 0.495, Test Loss: 1039730.062, Test R²: 0.372\n",
      "Epoch: 382, Train Loss: 871091.312, Train R²: 0.495, Test Loss: 1039846.312, Test R²: 0.372\n",
      "Epoch: 383, Train Loss: 869860.562, Train R²: 0.496, Test Loss: 1040470.625, Test R²: 0.371\n",
      "Epoch: 384, Train Loss: 868706.000, Train R²: 0.497, Test Loss: 1042528.562, Test R²: 0.370\n",
      "Epoch: 385, Train Loss: 867535.500, Train R²: 0.497, Test Loss: 1041625.688, Test R²: 0.371\n",
      "Epoch: 386, Train Loss: 866270.812, Train R²: 0.498, Test Loss: 1044192.062, Test R²: 0.369\n",
      "Epoch: 387, Train Loss: 864945.250, Train R²: 0.499, Test Loss: 1042627.312, Test R²: 0.370\n",
      "Epoch: 388, Train Loss: 863517.125, Train R²: 0.500, Test Loss: 1045107.688, Test R²: 0.369\n",
      "Epoch: 389, Train Loss: 862089.000, Train R²: 0.501, Test Loss: 1044721.750, Test R²: 0.369\n",
      "Epoch: 390, Train Loss: 860743.812, Train R²: 0.501, Test Loss: 1045787.375, Test R²: 0.368\n",
      "Epoch: 391, Train Loss: 859477.250, Train R²: 0.502, Test Loss: 1046832.625, Test R²: 0.367\n",
      "Epoch: 392, Train Loss: 858254.375, Train R²: 0.503, Test Loss: 1045968.750, Test R²: 0.368\n",
      "Epoch: 393, Train Loss: 857032.812, Train R²: 0.504, Test Loss: 1048308.125, Test R²: 0.367\n",
      "Epoch: 394, Train Loss: 855735.500, Train R²: 0.504, Test Loss: 1046710.375, Test R²: 0.368\n",
      "Epoch: 395, Train Loss: 854311.812, Train R²: 0.505, Test Loss: 1049504.250, Test R²: 0.366\n",
      "Epoch: 396, Train Loss: 852805.688, Train R²: 0.506, Test Loss: 1048234.188, Test R²: 0.367\n",
      "Epoch: 397, Train Loss: 851328.750, Train R²: 0.507, Test Loss: 1049923.125, Test R²: 0.366\n",
      "Epoch: 398, Train Loss: 849955.188, Train R²: 0.508, Test Loss: 1050871.500, Test R²: 0.365\n",
      "Epoch: 399, Train Loss: 848659.500, Train R²: 0.508, Test Loss: 1051323.750, Test R²: 0.365\n",
      "Epoch: 400, Train Loss: 847385.500, Train R²: 0.509, Test Loss: 1053936.250, Test R²: 0.363\n",
      "Epoch: 401, Train Loss: 846163.625, Train R²: 0.510, Test Loss: 1052826.500, Test R²: 0.364\n",
      "Epoch: 402, Train Loss: 844926.312, Train R²: 0.511, Test Loss: 1056815.375, Test R²: 0.361\n",
      "Epoch: 403, Train Loss: 843676.000, Train R²: 0.511, Test Loss: 1054170.500, Test R²: 0.363\n",
      "Epoch: 404, Train Loss: 842209.812, Train R²: 0.512, Test Loss: 1058034.000, Test R²: 0.361\n",
      "Epoch: 405, Train Loss: 840546.125, Train R²: 0.513, Test Loss: 1055320.625, Test R²: 0.362\n",
      "Epoch: 406, Train Loss: 838984.188, Train R²: 0.514, Test Loss: 1057053.750, Test R²: 0.361\n",
      "Epoch: 407, Train Loss: 837680.188, Train R²: 0.515, Test Loss: 1057584.250, Test R²: 0.361\n",
      "Epoch: 408, Train Loss: 836539.500, Train R²: 0.515, Test Loss: 1056966.875, Test R²: 0.361\n",
      "Epoch: 409, Train Loss: 835419.125, Train R²: 0.516, Test Loss: 1059954.750, Test R²: 0.360\n",
      "Epoch: 410, Train Loss: 834179.375, Train R²: 0.517, Test Loss: 1057902.250, Test R²: 0.361\n",
      "Epoch: 411, Train Loss: 832773.188, Train R²: 0.518, Test Loss: 1061346.500, Test R²: 0.359\n",
      "Epoch: 412, Train Loss: 831218.312, Train R²: 0.518, Test Loss: 1059389.875, Test R²: 0.360\n",
      "Epoch: 413, Train Loss: 829676.688, Train R²: 0.519, Test Loss: 1061478.875, Test R²: 0.359\n",
      "Epoch: 414, Train Loss: 828286.375, Train R²: 0.520, Test Loss: 1062046.500, Test R²: 0.358\n",
      "Epoch: 415, Train Loss: 827010.188, Train R²: 0.521, Test Loss: 1062068.000, Test R²: 0.358\n",
      "Epoch: 416, Train Loss: 825805.062, Train R²: 0.522, Test Loss: 1064792.125, Test R²: 0.357\n",
      "Epoch: 417, Train Loss: 824621.438, Train R²: 0.522, Test Loss: 1063239.500, Test R²: 0.358\n",
      "Epoch: 418, Train Loss: 823322.062, Train R²: 0.523, Test Loss: 1067103.000, Test R²: 0.355\n",
      "Epoch: 419, Train Loss: 821879.562, Train R²: 0.524, Test Loss: 1065036.875, Test R²: 0.356\n",
      "Epoch: 420, Train Loss: 820307.625, Train R²: 0.525, Test Loss: 1069251.250, Test R²: 0.354\n",
      "Epoch: 421, Train Loss: 818702.812, Train R²: 0.526, Test Loss: 1068237.250, Test R²: 0.355\n",
      "Epoch: 422, Train Loss: 817164.625, Train R²: 0.527, Test Loss: 1070691.125, Test R²: 0.353\n",
      "Epoch: 423, Train Loss: 815695.562, Train R²: 0.527, Test Loss: 1070808.375, Test R²: 0.353\n",
      "Epoch: 424, Train Loss: 814268.438, Train R²: 0.528, Test Loss: 1071813.125, Test R²: 0.352\n",
      "Epoch: 425, Train Loss: 812873.938, Train R²: 0.529, Test Loss: 1073431.500, Test R²: 0.351\n",
      "Epoch: 426, Train Loss: 811489.500, Train R²: 0.530, Test Loss: 1072887.625, Test R²: 0.352\n",
      "Epoch: 427, Train Loss: 810175.625, Train R²: 0.531, Test Loss: 1075341.500, Test R²: 0.350\n",
      "Epoch: 428, Train Loss: 808985.125, Train R²: 0.531, Test Loss: 1073095.375, Test R²: 0.352\n",
      "Epoch: 429, Train Loss: 807988.688, Train R²: 0.532, Test Loss: 1078073.250, Test R²: 0.349\n",
      "Epoch: 430, Train Loss: 807245.750, Train R²: 0.532, Test Loss: 1073630.750, Test R²: 0.351\n",
      "Epoch: 431, Train Loss: 806241.188, Train R²: 0.533, Test Loss: 1081067.375, Test R²: 0.347\n",
      "Epoch: 432, Train Loss: 804255.062, Train R²: 0.534, Test Loss: 1074414.375, Test R²: 0.351\n",
      "Epoch: 433, Train Loss: 801639.625, Train R²: 0.536, Test Loss: 1078164.375, Test R²: 0.349\n",
      "Epoch: 434, Train Loss: 800049.438, Train R²: 0.537, Test Loss: 1078074.625, Test R²: 0.349\n",
      "Epoch: 435, Train Loss: 799588.812, Train R²: 0.537, Test Loss: 1076419.625, Test R²: 0.350\n",
      "Epoch: 436, Train Loss: 798690.750, Train R²: 0.537, Test Loss: 1082882.875, Test R²: 0.346\n",
      "Epoch: 437, Train Loss: 796630.812, Train R²: 0.539, Test Loss: 1078197.250, Test R²: 0.349\n",
      "Epoch: 438, Train Loss: 794540.562, Train R²: 0.540, Test Loss: 1080725.500, Test R²: 0.347\n",
      "Epoch: 439, Train Loss: 793535.125, Train R²: 0.540, Test Loss: 1082737.500, Test R²: 0.346\n",
      "Epoch: 440, Train Loss: 792862.938, Train R²: 0.541, Test Loss: 1079809.625, Test R²: 0.348\n",
      "Epoch: 441, Train Loss: 791422.625, Train R²: 0.542, Test Loss: 1085383.375, Test R²: 0.344\n",
      "Epoch: 442, Train Loss: 789317.688, Train R²: 0.543, Test Loss: 1081724.000, Test R²: 0.346\n",
      "Epoch: 443, Train Loss: 787729.062, Train R²: 0.544, Test Loss: 1083218.625, Test R²: 0.345\n",
      "Epoch: 444, Train Loss: 786861.375, Train R²: 0.544, Test Loss: 1087215.875, Test R²: 0.343\n",
      "Epoch: 445, Train Loss: 785837.062, Train R²: 0.545, Test Loss: 1084327.625, Test R²: 0.345\n",
      "Epoch: 446, Train Loss: 784165.062, Train R²: 0.546, Test Loss: 1089277.625, Test R²: 0.342\n",
      "Epoch: 447, Train Loss: 782446.375, Train R²: 0.547, Test Loss: 1087633.625, Test R²: 0.343\n",
      "Epoch: 448, Train Loss: 781238.750, Train R²: 0.547, Test Loss: 1088264.750, Test R²: 0.342\n",
      "Epoch: 449, Train Loss: 780315.000, Train R²: 0.548, Test Loss: 1092661.875, Test R²: 0.340\n",
      "Epoch: 450, Train Loss: 779112.438, Train R²: 0.549, Test Loss: 1089619.625, Test R²: 0.342\n",
      "Epoch: 451, Train Loss: 777508.188, Train R²: 0.550, Test Loss: 1094520.750, Test R²: 0.339\n",
      "Epoch: 452, Train Loss: 775857.750, Train R²: 0.551, Test Loss: 1093211.375, Test R²: 0.339\n",
      "Epoch: 453, Train Loss: 774519.312, Train R²: 0.551, Test Loss: 1094157.125, Test R²: 0.339\n",
      "Epoch: 454, Train Loss: 773417.688, Train R²: 0.552, Test Loss: 1096741.375, Test R²: 0.337\n",
      "Epoch: 455, Train Loss: 772275.250, Train R²: 0.553, Test Loss: 1094107.125, Test R²: 0.339\n",
      "Epoch: 456, Train Loss: 770909.500, Train R²: 0.553, Test Loss: 1098683.000, Test R²: 0.336\n",
      "Epoch: 457, Train Loss: 769384.312, Train R²: 0.554, Test Loss: 1096217.500, Test R²: 0.338\n",
      "Epoch: 458, Train Loss: 767891.000, Train R²: 0.555, Test Loss: 1098459.250, Test R²: 0.336\n",
      "Epoch: 459, Train Loss: 766528.875, Train R²: 0.556, Test Loss: 1098659.375, Test R²: 0.336\n",
      "Epoch: 460, Train Loss: 765285.375, Train R²: 0.557, Test Loss: 1098971.500, Test R²: 0.336\n",
      "Epoch: 461, Train Loss: 764086.688, Train R²: 0.557, Test Loss: 1103272.250, Test R²: 0.333\n",
      "Epoch: 462, Train Loss: 762874.375, Train R²: 0.558, Test Loss: 1101842.750, Test R²: 0.334\n",
      "Epoch: 463, Train Loss: 761618.062, Train R²: 0.559, Test Loss: 1106702.125, Test R²: 0.331\n",
      "Epoch: 464, Train Loss: 760297.688, Train R²: 0.560, Test Loss: 1103684.625, Test R²: 0.333\n",
      "Epoch: 465, Train Loss: 758932.938, Train R²: 0.560, Test Loss: 1108553.375, Test R²: 0.330\n",
      "Epoch: 466, Train Loss: 757553.375, Train R²: 0.561, Test Loss: 1105993.000, Test R²: 0.332\n",
      "Epoch: 467, Train Loss: 756176.375, Train R²: 0.562, Test Loss: 1110189.625, Test R²: 0.329\n",
      "Epoch: 468, Train Loss: 754805.312, Train R²: 0.563, Test Loss: 1108015.000, Test R²: 0.331\n",
      "Epoch: 469, Train Loss: 753428.625, Train R²: 0.564, Test Loss: 1111435.000, Test R²: 0.328\n",
      "Epoch: 470, Train Loss: 752070.125, Train R²: 0.564, Test Loss: 1110465.500, Test R²: 0.329\n",
      "Epoch: 471, Train Loss: 750734.562, Train R²: 0.565, Test Loss: 1113752.125, Test R²: 0.327\n",
      "Epoch: 472, Train Loss: 749416.438, Train R²: 0.566, Test Loss: 1112953.750, Test R²: 0.328\n",
      "Epoch: 473, Train Loss: 748113.000, Train R²: 0.567, Test Loss: 1115645.625, Test R²: 0.326\n",
      "Epoch: 474, Train Loss: 746840.438, Train R²: 0.567, Test Loss: 1114352.000, Test R²: 0.327\n",
      "Epoch: 475, Train Loss: 745625.000, Train R²: 0.568, Test Loss: 1118204.500, Test R²: 0.324\n",
      "Epoch: 476, Train Loss: 744508.312, Train R²: 0.569, Test Loss: 1115881.500, Test R²: 0.326\n",
      "Epoch: 477, Train Loss: 743605.688, Train R²: 0.569, Test Loss: 1122203.250, Test R²: 0.322\n",
      "Epoch: 478, Train Loss: 743064.438, Train R²: 0.570, Test Loss: 1117229.750, Test R²: 0.325\n",
      "Epoch: 479, Train Loss: 742937.812, Train R²: 0.570, Test Loss: 1128553.500, Test R²: 0.318\n",
      "Epoch: 480, Train Loss: 742452.062, Train R²: 0.570, Test Loss: 1119569.125, Test R²: 0.324\n",
      "Epoch: 481, Train Loss: 740473.125, Train R²: 0.571, Test Loss: 1131015.500, Test R²: 0.317\n",
      "Epoch: 482, Train Loss: 737249.062, Train R²: 0.573, Test Loss: 1121762.500, Test R²: 0.322\n",
      "Epoch: 483, Train Loss: 735213.250, Train R²: 0.574, Test Loss: 1124331.875, Test R²: 0.321\n",
      "Epoch: 484, Train Loss: 735033.125, Train R²: 0.574, Test Loss: 1130046.500, Test R²: 0.317\n",
      "Epoch: 485, Train Loss: 734852.188, Train R²: 0.574, Test Loss: 1124448.250, Test R²: 0.321\n",
      "Epoch: 486, Train Loss: 733122.438, Train R²: 0.575, Test Loss: 1133199.250, Test R²: 0.315\n",
      "Epoch: 487, Train Loss: 730659.375, Train R²: 0.577, Test Loss: 1127374.250, Test R²: 0.319\n",
      "Epoch: 488, Train Loss: 729368.125, Train R²: 0.577, Test Loss: 1128652.500, Test R²: 0.318\n",
      "Epoch: 489, Train Loss: 729095.000, Train R²: 0.578, Test Loss: 1135167.625, Test R²: 0.314\n",
      "Epoch: 490, Train Loss: 728287.250, Train R²: 0.578, Test Loss: 1129748.250, Test R²: 0.317\n",
      "Epoch: 491, Train Loss: 726354.688, Train R²: 0.579, Test Loss: 1136298.375, Test R²: 0.313\n",
      "Epoch: 492, Train Loss: 724513.500, Train R²: 0.580, Test Loss: 1133233.125, Test R²: 0.315\n",
      "Epoch: 493, Train Loss: 723582.875, Train R²: 0.581, Test Loss: 1132794.125, Test R²: 0.316\n",
      "Epoch: 494, Train Loss: 722970.125, Train R²: 0.581, Test Loss: 1138615.625, Test R²: 0.312\n",
      "Epoch: 495, Train Loss: 721798.062, Train R²: 0.582, Test Loss: 1133406.750, Test R²: 0.315\n",
      "Epoch: 496, Train Loss: 720053.812, Train R²: 0.583, Test Loss: 1137975.125, Test R²: 0.312\n",
      "Epoch: 497, Train Loss: 718509.812, Train R²: 0.584, Test Loss: 1135557.625, Test R²: 0.314\n",
      "Epoch: 498, Train Loss: 717475.562, Train R²: 0.584, Test Loss: 1134745.875, Test R²: 0.314\n",
      "Epoch: 499, Train Loss: 716653.875, Train R²: 0.585, Test Loss: 1139207.500, Test R²: 0.312\n",
      "Epoch: 500, Train Loss: 715555.875, Train R²: 0.585, Test Loss: 1134375.750, Test R²: 0.315\n",
      "Epoch: 501, Train Loss: 714105.938, Train R²: 0.586, Test Loss: 1138777.625, Test R²: 0.312\n",
      "Epoch: 502, Train Loss: 712619.000, Train R²: 0.587, Test Loss: 1135754.000, Test R²: 0.314\n",
      "Epoch: 503, Train Loss: 711376.312, Train R²: 0.588, Test Loss: 1136354.625, Test R²: 0.313\n",
      "Epoch: 504, Train Loss: 710375.812, Train R²: 0.588, Test Loss: 1138374.625, Test R²: 0.312\n",
      "Epoch: 505, Train Loss: 709405.562, Train R²: 0.589, Test Loss: 1134934.875, Test R²: 0.314\n",
      "Epoch: 506, Train Loss: 708323.000, Train R²: 0.590, Test Loss: 1139221.125, Test R²: 0.312\n",
      "Epoch: 507, Train Loss: 707064.125, Train R²: 0.590, Test Loss: 1135245.875, Test R²: 0.314\n",
      "Epoch: 508, Train Loss: 705700.875, Train R²: 0.591, Test Loss: 1139007.750, Test R²: 0.312\n",
      "Epoch: 509, Train Loss: 704388.125, Train R²: 0.592, Test Loss: 1137427.875, Test R²: 0.313\n",
      "Epoch: 510, Train Loss: 703201.938, Train R²: 0.593, Test Loss: 1138663.875, Test R²: 0.312\n",
      "Epoch: 511, Train Loss: 702114.375, Train R²: 0.593, Test Loss: 1140876.375, Test R²: 0.311\n",
      "Epoch: 512, Train Loss: 701084.812, Train R²: 0.594, Test Loss: 1139870.750, Test R²: 0.311\n",
      "Epoch: 513, Train Loss: 700067.562, Train R²: 0.594, Test Loss: 1144409.875, Test R²: 0.309\n",
      "Epoch: 514, Train Loss: 699061.250, Train R²: 0.595, Test Loss: 1141546.625, Test R²: 0.310\n",
      "Epoch: 515, Train Loss: 698015.500, Train R²: 0.596, Test Loss: 1147424.875, Test R²: 0.307\n",
      "Epoch: 516, Train Loss: 696902.688, Train R²: 0.596, Test Loss: 1143443.750, Test R²: 0.309\n",
      "Epoch: 517, Train Loss: 695695.000, Train R²: 0.597, Test Loss: 1148782.625, Test R²: 0.306\n",
      "Epoch: 518, Train Loss: 694426.750, Train R²: 0.598, Test Loss: 1144500.625, Test R²: 0.308\n",
      "Epoch: 519, Train Loss: 693119.625, Train R²: 0.598, Test Loss: 1148670.625, Test R²: 0.306\n",
      "Epoch: 520, Train Loss: 691842.812, Train R²: 0.599, Test Loss: 1146199.250, Test R²: 0.307\n",
      "Epoch: 521, Train Loss: 690598.812, Train R²: 0.600, Test Loss: 1148889.625, Test R²: 0.306\n",
      "Epoch: 522, Train Loss: 689380.375, Train R²: 0.601, Test Loss: 1148173.250, Test R²: 0.306\n",
      "Epoch: 523, Train Loss: 688191.438, Train R²: 0.601, Test Loss: 1149515.250, Test R²: 0.305\n",
      "Epoch: 524, Train Loss: 687026.812, Train R²: 0.602, Test Loss: 1150778.875, Test R²: 0.305\n",
      "Epoch: 525, Train Loss: 685885.625, Train R²: 0.603, Test Loss: 1150650.750, Test R²: 0.305\n",
      "Epoch: 526, Train Loss: 684766.688, Train R²: 0.603, Test Loss: 1152744.875, Test R²: 0.303\n",
      "Epoch: 527, Train Loss: 683701.688, Train R²: 0.604, Test Loss: 1150602.750, Test R²: 0.305\n",
      "Epoch: 528, Train Loss: 682774.125, Train R²: 0.604, Test Loss: 1155079.500, Test R²: 0.302\n",
      "Epoch: 529, Train Loss: 682116.625, Train R²: 0.605, Test Loss: 1150664.375, Test R²: 0.305\n",
      "Epoch: 530, Train Loss: 681939.812, Train R²: 0.605, Test Loss: 1160217.875, Test R²: 0.299\n",
      "Epoch: 531, Train Loss: 682249.125, Train R²: 0.605, Test Loss: 1151520.250, Test R²: 0.304\n",
      "Epoch: 532, Train Loss: 682462.438, Train R²: 0.605, Test Loss: 1167348.125, Test R²: 0.295\n",
      "Epoch: 533, Train Loss: 680973.188, Train R²: 0.606, Test Loss: 1153248.625, Test R²: 0.303\n",
      "Epoch: 534, Train Loss: 677466.875, Train R²: 0.608, Test Loss: 1163565.000, Test R²: 0.297\n",
      "Epoch: 535, Train Loss: 674532.062, Train R²: 0.609, Test Loss: 1156615.125, Test R²: 0.301\n",
      "Epoch: 536, Train Loss: 674130.438, Train R²: 0.609, Test Loss: 1155666.375, Test R²: 0.302\n",
      "Epoch: 537, Train Loss: 674765.500, Train R²: 0.609, Test Loss: 1166211.625, Test R²: 0.295\n",
      "Epoch: 538, Train Loss: 673732.062, Train R²: 0.610, Test Loss: 1156375.875, Test R²: 0.301\n",
      "Epoch: 539, Train Loss: 671072.875, Train R²: 0.611, Test Loss: 1163925.875, Test R²: 0.297\n",
      "Epoch: 540, Train Loss: 669168.125, Train R²: 0.612, Test Loss: 1160707.250, Test R²: 0.299\n",
      "Epoch: 541, Train Loss: 668977.625, Train R²: 0.612, Test Loss: 1158991.125, Test R²: 0.300\n",
      "Epoch: 542, Train Loss: 668793.688, Train R²: 0.613, Test Loss: 1168459.375, Test R²: 0.294\n",
      "Epoch: 543, Train Loss: 667171.125, Train R²: 0.614, Test Loss: 1160661.250, Test R²: 0.299\n",
      "Epoch: 544, Train Loss: 665134.062, Train R²: 0.615, Test Loss: 1165184.125, Test R²: 0.296\n",
      "Epoch: 545, Train Loss: 664138.062, Train R²: 0.615, Test Loss: 1165963.000, Test R²: 0.296\n",
      "Epoch: 546, Train Loss: 663846.750, Train R²: 0.615, Test Loss: 1162806.625, Test R²: 0.297\n",
      "Epoch: 547, Train Loss: 663045.875, Train R²: 0.616, Test Loss: 1170949.000, Test R²: 0.292\n",
      "Epoch: 548, Train Loss: 661401.000, Train R²: 0.617, Test Loss: 1164966.750, Test R²: 0.296\n",
      "Epoch: 549, Train Loss: 659868.188, Train R²: 0.618, Test Loss: 1168396.000, Test R²: 0.294\n",
      "Epoch: 550, Train Loss: 659028.625, Train R²: 0.618, Test Loss: 1171083.750, Test R²: 0.292\n",
      "Epoch: 551, Train Loss: 658451.938, Train R²: 0.619, Test Loss: 1168559.250, Test R²: 0.294\n",
      "Epoch: 552, Train Loss: 657498.875, Train R²: 0.619, Test Loss: 1175340.375, Test R²: 0.290\n",
      "Epoch: 553, Train Loss: 656082.250, Train R²: 0.620, Test Loss: 1170329.000, Test R²: 0.293\n",
      "Epoch: 554, Train Loss: 654758.938, Train R²: 0.621, Test Loss: 1173173.375, Test R²: 0.291\n",
      "Epoch: 555, Train Loss: 653809.688, Train R²: 0.621, Test Loss: 1175016.875, Test R²: 0.290\n",
      "Epoch: 556, Train Loss: 653057.375, Train R²: 0.622, Test Loss: 1173397.750, Test R²: 0.291\n",
      "Epoch: 557, Train Loss: 652149.438, Train R²: 0.622, Test Loss: 1178967.375, Test R²: 0.288\n",
      "Epoch: 558, Train Loss: 650953.188, Train R²: 0.623, Test Loss: 1175302.750, Test R²: 0.290\n",
      "Epoch: 559, Train Loss: 649705.188, Train R²: 0.624, Test Loss: 1179140.250, Test R²: 0.288\n",
      "Epoch: 560, Train Loss: 648603.375, Train R²: 0.624, Test Loss: 1179736.000, Test R²: 0.287\n",
      "Epoch: 561, Train Loss: 647680.438, Train R²: 0.625, Test Loss: 1179844.625, Test R²: 0.287\n",
      "Epoch: 562, Train Loss: 646800.625, Train R²: 0.625, Test Loss: 1183823.375, Test R²: 0.285\n",
      "Epoch: 563, Train Loss: 645825.875, Train R²: 0.626, Test Loss: 1180760.000, Test R²: 0.287\n",
      "Epoch: 564, Train Loss: 644733.188, Train R²: 0.627, Test Loss: 1185198.875, Test R²: 0.284\n",
      "Epoch: 565, Train Loss: 643558.125, Train R²: 0.627, Test Loss: 1182615.625, Test R²: 0.285\n",
      "Epoch: 566, Train Loss: 642392.125, Train R²: 0.628, Test Loss: 1185252.500, Test R²: 0.284\n",
      "Epoch: 567, Train Loss: 641291.125, Train R²: 0.628, Test Loss: 1185271.750, Test R²: 0.284\n",
      "Epoch: 568, Train Loss: 640253.312, Train R²: 0.629, Test Loss: 1185433.500, Test R²: 0.284\n",
      "Epoch: 569, Train Loss: 639258.562, Train R²: 0.630, Test Loss: 1187988.625, Test R²: 0.282\n",
      "Epoch: 570, Train Loss: 638267.500, Train R²: 0.630, Test Loss: 1186081.500, Test R²: 0.283\n",
      "Epoch: 571, Train Loss: 637274.000, Train R²: 0.631, Test Loss: 1190018.000, Test R²: 0.281\n",
      "Epoch: 572, Train Loss: 636262.750, Train R²: 0.631, Test Loss: 1187121.625, Test R²: 0.283\n",
      "Epoch: 573, Train Loss: 635232.688, Train R²: 0.632, Test Loss: 1191723.875, Test R²: 0.280\n",
      "Epoch: 574, Train Loss: 634174.375, Train R²: 0.633, Test Loss: 1188571.250, Test R²: 0.282\n",
      "Epoch: 575, Train Loss: 633115.625, Train R²: 0.633, Test Loss: 1192953.125, Test R²: 0.279\n",
      "Epoch: 576, Train Loss: 632051.250, Train R²: 0.634, Test Loss: 1189913.500, Test R²: 0.281\n",
      "Epoch: 577, Train Loss: 631007.562, Train R²: 0.634, Test Loss: 1194334.375, Test R²: 0.278\n",
      "Epoch: 578, Train Loss: 629976.188, Train R²: 0.635, Test Loss: 1191595.000, Test R²: 0.280\n",
      "Epoch: 579, Train Loss: 628971.688, Train R²: 0.636, Test Loss: 1195856.750, Test R²: 0.277\n",
      "Epoch: 580, Train Loss: 627978.375, Train R²: 0.636, Test Loss: 1192617.625, Test R²: 0.279\n",
      "Epoch: 581, Train Loss: 627012.688, Train R²: 0.637, Test Loss: 1197601.500, Test R²: 0.276\n",
      "Epoch: 582, Train Loss: 626104.188, Train R²: 0.637, Test Loss: 1193759.000, Test R²: 0.279\n",
      "Epoch: 583, Train Loss: 625297.188, Train R²: 0.638, Test Loss: 1200239.250, Test R²: 0.275\n",
      "Epoch: 584, Train Loss: 624606.375, Train R²: 0.638, Test Loss: 1194383.750, Test R²: 0.278\n",
      "Epoch: 585, Train Loss: 624110.375, Train R²: 0.638, Test Loss: 1203669.125, Test R²: 0.273\n",
      "Epoch: 586, Train Loss: 623642.688, Train R²: 0.639, Test Loss: 1195103.375, Test R²: 0.278\n",
      "Epoch: 587, Train Loss: 623105.562, Train R²: 0.639, Test Loss: 1207428.000, Test R²: 0.270\n",
      "Epoch: 588, Train Loss: 622168.438, Train R²: 0.640, Test Loss: 1196408.250, Test R²: 0.277\n",
      "Epoch: 589, Train Loss: 620581.688, Train R²: 0.640, Test Loss: 1208087.625, Test R²: 0.270\n",
      "Epoch: 590, Train Loss: 618596.188, Train R²: 0.642, Test Loss: 1199278.875, Test R²: 0.275\n",
      "Epoch: 591, Train Loss: 616780.750, Train R²: 0.643, Test Loss: 1205193.625, Test R²: 0.272\n",
      "Epoch: 592, Train Loss: 615590.688, Train R²: 0.643, Test Loss: 1204744.250, Test R²: 0.272\n",
      "Epoch: 593, Train Loss: 614986.875, Train R²: 0.644, Test Loss: 1203684.500, Test R²: 0.273\n",
      "Epoch: 594, Train Loss: 614614.625, Train R²: 0.644, Test Loss: 1211610.500, Test R²: 0.268\n",
      "Epoch: 595, Train Loss: 614029.500, Train R²: 0.644, Test Loss: 1205299.000, Test R²: 0.272\n",
      "Epoch: 596, Train Loss: 612991.312, Train R²: 0.645, Test Loss: 1214826.625, Test R²: 0.266\n",
      "Epoch: 597, Train Loss: 611525.812, Train R²: 0.646, Test Loss: 1207802.375, Test R²: 0.270\n",
      "Epoch: 598, Train Loss: 609974.688, Train R²: 0.647, Test Loss: 1213488.500, Test R²: 0.267\n",
      "Epoch: 599, Train Loss: 608666.875, Train R²: 0.647, Test Loss: 1211715.625, Test R²: 0.268\n",
      "Epoch: 600, Train Loss: 607720.625, Train R²: 0.648, Test Loss: 1211698.250, Test R²: 0.268\n",
      "Epoch: 601, Train Loss: 607013.188, Train R²: 0.648, Test Loss: 1215118.500, Test R²: 0.266\n",
      "Epoch: 602, Train Loss: 606339.438, Train R²: 0.649, Test Loss: 1210929.875, Test R²: 0.268\n",
      "Epoch: 603, Train Loss: 605555.375, Train R²: 0.649, Test Loss: 1216696.375, Test R²: 0.265\n",
      "Epoch: 604, Train Loss: 604544.500, Train R²: 0.650, Test Loss: 1211054.625, Test R²: 0.268\n",
      "Epoch: 605, Train Loss: 603314.000, Train R²: 0.650, Test Loss: 1216799.375, Test R²: 0.265\n",
      "Epoch: 606, Train Loss: 601997.375, Train R²: 0.651, Test Loss: 1212576.250, Test R²: 0.267\n",
      "Epoch: 607, Train Loss: 600745.625, Train R²: 0.652, Test Loss: 1216188.250, Test R²: 0.265\n",
      "Epoch: 608, Train Loss: 599613.625, Train R²: 0.653, Test Loss: 1215013.375, Test R²: 0.266\n",
      "Epoch: 609, Train Loss: 598592.438, Train R²: 0.653, Test Loss: 1216120.625, Test R²: 0.265\n",
      "Epoch: 610, Train Loss: 597650.812, Train R²: 0.654, Test Loss: 1218157.000, Test R²: 0.264\n",
      "Epoch: 611, Train Loss: 596775.438, Train R²: 0.654, Test Loss: 1217486.125, Test R²: 0.264\n",
      "Epoch: 612, Train Loss: 595972.062, Train R²: 0.655, Test Loss: 1221708.875, Test R²: 0.262\n",
      "Epoch: 613, Train Loss: 595252.625, Train R²: 0.655, Test Loss: 1218507.625, Test R²: 0.264\n",
      "Epoch: 614, Train Loss: 594646.375, Train R²: 0.656, Test Loss: 1225115.500, Test R²: 0.260\n",
      "Epoch: 615, Train Loss: 594149.312, Train R²: 0.656, Test Loss: 1219682.875, Test R²: 0.263\n",
      "Epoch: 616, Train Loss: 593735.625, Train R²: 0.656, Test Loss: 1228787.125, Test R²: 0.258\n",
      "Epoch: 617, Train Loss: 593114.625, Train R²: 0.656, Test Loss: 1220378.750, Test R²: 0.263\n",
      "Epoch: 618, Train Loss: 592218.062, Train R²: 0.657, Test Loss: 1230245.750, Test R²: 0.257\n",
      "Epoch: 619, Train Loss: 590661.750, Train R²: 0.658, Test Loss: 1221266.125, Test R²: 0.262\n",
      "Epoch: 620, Train Loss: 588814.125, Train R²: 0.659, Test Loss: 1228236.125, Test R²: 0.258\n",
      "Epoch: 621, Train Loss: 587082.375, Train R²: 0.660, Test Loss: 1223713.000, Test R²: 0.261\n",
      "Epoch: 622, Train Loss: 585885.062, Train R²: 0.661, Test Loss: 1225589.250, Test R²: 0.259\n",
      "Epoch: 623, Train Loss: 585250.875, Train R²: 0.661, Test Loss: 1228680.125, Test R²: 0.258\n",
      "Epoch: 624, Train Loss: 584892.062, Train R²: 0.661, Test Loss: 1225253.375, Test R²: 0.260\n",
      "Epoch: 625, Train Loss: 584455.562, Train R²: 0.661, Test Loss: 1232744.500, Test R²: 0.255\n",
      "Epoch: 626, Train Loss: 583578.375, Train R²: 0.662, Test Loss: 1225868.750, Test R²: 0.259\n",
      "Epoch: 627, Train Loss: 582281.125, Train R²: 0.663, Test Loss: 1233115.375, Test R²: 0.255\n",
      "Epoch: 628, Train Loss: 580705.125, Train R²: 0.664, Test Loss: 1227240.875, Test R²: 0.258\n",
      "Epoch: 629, Train Loss: 579230.812, Train R²: 0.664, Test Loss: 1230914.875, Test R²: 0.256\n",
      "Epoch: 630, Train Loss: 578105.500, Train R²: 0.665, Test Loss: 1230515.125, Test R²: 0.256\n",
      "Epoch: 631, Train Loss: 577306.938, Train R²: 0.666, Test Loss: 1229959.000, Test R²: 0.257\n",
      "Epoch: 632, Train Loss: 576662.500, Train R²: 0.666, Test Loss: 1234694.250, Test R²: 0.254\n",
      "Epoch: 633, Train Loss: 575991.375, Train R²: 0.666, Test Loss: 1230325.375, Test R²: 0.257\n",
      "Epoch: 634, Train Loss: 575193.625, Train R²: 0.667, Test Loss: 1236716.500, Test R²: 0.253\n",
      "Epoch: 635, Train Loss: 574220.125, Train R²: 0.667, Test Loss: 1230751.750, Test R²: 0.256\n",
      "Epoch: 636, Train Loss: 573109.812, Train R²: 0.668, Test Loss: 1237204.250, Test R²: 0.252\n",
      "Epoch: 637, Train Loss: 571919.375, Train R²: 0.669, Test Loss: 1232299.000, Test R²: 0.255\n",
      "Epoch: 638, Train Loss: 570724.188, Train R²: 0.669, Test Loss: 1237601.250, Test R²: 0.252\n",
      "Epoch: 639, Train Loss: 569550.500, Train R²: 0.670, Test Loss: 1234790.750, Test R²: 0.254\n",
      "Epoch: 640, Train Loss: 568444.812, Train R²: 0.671, Test Loss: 1238422.125, Test R²: 0.252\n",
      "Epoch: 641, Train Loss: 567387.375, Train R²: 0.671, Test Loss: 1237714.375, Test R²: 0.252\n",
      "Epoch: 642, Train Loss: 566374.688, Train R²: 0.672, Test Loss: 1239684.875, Test R²: 0.251\n",
      "Epoch: 643, Train Loss: 565402.125, Train R²: 0.672, Test Loss: 1240390.875, Test R²: 0.251\n",
      "Epoch: 644, Train Loss: 564459.438, Train R²: 0.673, Test Loss: 1241103.625, Test R²: 0.250\n",
      "Epoch: 645, Train Loss: 563531.250, Train R²: 0.674, Test Loss: 1242657.125, Test R²: 0.249\n",
      "Epoch: 646, Train Loss: 562625.125, Train R²: 0.674, Test Loss: 1242397.375, Test R²: 0.249\n",
      "Epoch: 647, Train Loss: 561748.500, Train R²: 0.675, Test Loss: 1245640.875, Test R²: 0.247\n",
      "Epoch: 648, Train Loss: 560956.312, Train R²: 0.675, Test Loss: 1244270.000, Test R²: 0.248\n",
      "Epoch: 649, Train Loss: 560357.125, Train R²: 0.675, Test Loss: 1249612.625, Test R²: 0.245\n",
      "Epoch: 650, Train Loss: 560175.375, Train R²: 0.675, Test Loss: 1244982.375, Test R²: 0.248\n",
      "Epoch: 651, Train Loss: 560905.500, Train R²: 0.675, Test Loss: 1255794.625, Test R²: 0.241\n",
      "Epoch: 652, Train Loss: 563040.875, Train R²: 0.674, Test Loss: 1246382.625, Test R²: 0.247\n",
      "Epoch: 653, Train Loss: 566439.500, Train R²: 0.672, Test Loss: 1268404.750, Test R²: 0.234\n",
      "Epoch: 654, Train Loss: 566870.375, Train R²: 0.672, Test Loss: 1249396.375, Test R²: 0.245\n",
      "Epoch: 655, Train Loss: 561344.312, Train R²: 0.675, Test Loss: 1264167.875, Test R²: 0.236\n",
      "Epoch: 656, Train Loss: 554200.562, Train R²: 0.679, Test Loss: 1247809.875, Test R²: 0.246\n",
      "Epoch: 657, Train Loss: 553947.188, Train R²: 0.679, Test Loss: 1247916.125, Test R²: 0.246\n",
      "Epoch: 658, Train Loss: 557797.250, Train R²: 0.677, Test Loss: 1264026.250, Test R²: 0.236\n",
      "Epoch: 659, Train Loss: 556656.188, Train R²: 0.678, Test Loss: 1249304.750, Test R²: 0.245\n",
      "Epoch: 660, Train Loss: 551329.875, Train R²: 0.681, Test Loss: 1256244.625, Test R²: 0.241\n",
      "Epoch: 661, Train Loss: 549656.125, Train R²: 0.682, Test Loss: 1254205.250, Test R²: 0.242\n",
      "Epoch: 662, Train Loss: 551987.500, Train R²: 0.680, Test Loss: 1249783.750, Test R²: 0.245\n",
      "Epoch: 663, Train Loss: 551532.500, Train R²: 0.680, Test Loss: 1263535.750, Test R²: 0.237\n",
      "Epoch: 664, Train Loss: 547665.250, Train R²: 0.683, Test Loss: 1252236.000, Test R²: 0.243\n",
      "Epoch: 665, Train Loss: 546245.375, Train R²: 0.684, Test Loss: 1253559.000, Test R²: 0.243\n",
      "Epoch: 666, Train Loss: 547581.125, Train R²: 0.683, Test Loss: 1262947.125, Test R²: 0.237\n",
      "Epoch: 667, Train Loss: 546897.000, Train R²: 0.683, Test Loss: 1253935.875, Test R²: 0.242\n",
      "Epoch: 668, Train Loss: 544042.875, Train R²: 0.685, Test Loss: 1261596.250, Test R²: 0.238\n",
      "Epoch: 669, Train Loss: 542893.625, Train R²: 0.685, Test Loss: 1262278.750, Test R²: 0.237\n",
      "Epoch: 670, Train Loss: 543536.750, Train R²: 0.685, Test Loss: 1259356.750, Test R²: 0.239\n",
      "Epoch: 671, Train Loss: 542850.438, Train R²: 0.686, Test Loss: 1269409.625, Test R²: 0.233\n",
      "Epoch: 672, Train Loss: 540672.625, Train R²: 0.687, Test Loss: 1262488.250, Test R²: 0.237\n",
      "Epoch: 673, Train Loss: 539615.562, Train R²: 0.687, Test Loss: 1263664.125, Test R²: 0.236\n",
      "Epoch: 674, Train Loss: 539771.812, Train R²: 0.687, Test Loss: 1269484.125, Test R²: 0.233\n",
      "Epoch: 675, Train Loss: 539152.812, Train R²: 0.688, Test Loss: 1263652.375, Test R²: 0.236\n",
      "Epoch: 676, Train Loss: 537569.062, Train R²: 0.689, Test Loss: 1269590.125, Test R²: 0.233\n",
      "Epoch: 677, Train Loss: 536417.688, Train R²: 0.689, Test Loss: 1268603.375, Test R²: 0.233\n",
      "Epoch: 678, Train Loss: 536189.188, Train R²: 0.689, Test Loss: 1267192.500, Test R²: 0.234\n",
      "Epoch: 679, Train Loss: 535785.062, Train R²: 0.690, Test Loss: 1273868.000, Test R²: 0.230\n",
      "Epoch: 680, Train Loss: 534551.125, Train R²: 0.690, Test Loss: 1269725.250, Test R²: 0.233\n",
      "Epoch: 681, Train Loss: 533353.000, Train R²: 0.691, Test Loss: 1273021.375, Test R²: 0.231\n",
      "Epoch: 682, Train Loss: 532801.375, Train R²: 0.691, Test Loss: 1275536.875, Test R²: 0.229\n",
      "Epoch: 683, Train Loss: 532389.625, Train R²: 0.692, Test Loss: 1272830.375, Test R²: 0.231\n",
      "Epoch: 684, Train Loss: 531515.938, Train R²: 0.692, Test Loss: 1278253.625, Test R²: 0.228\n",
      "Epoch: 685, Train Loss: 530376.188, Train R²: 0.693, Test Loss: 1275461.375, Test R²: 0.229\n",
      "Epoch: 686, Train Loss: 529515.188, Train R²: 0.693, Test Loss: 1276808.375, Test R²: 0.229\n",
      "Epoch: 687, Train Loss: 528978.938, Train R²: 0.694, Test Loss: 1279899.375, Test R²: 0.227\n",
      "Epoch: 688, Train Loss: 528384.375, Train R²: 0.694, Test Loss: 1277495.875, Test R²: 0.228\n",
      "Epoch: 689, Train Loss: 527491.062, Train R²: 0.694, Test Loss: 1282459.000, Test R²: 0.225\n",
      "Epoch: 690, Train Loss: 526486.000, Train R²: 0.695, Test Loss: 1280735.500, Test R²: 0.226\n",
      "Epoch: 691, Train Loss: 525656.688, Train R²: 0.695, Test Loss: 1281955.125, Test R²: 0.225\n",
      "Epoch: 692, Train Loss: 525009.438, Train R²: 0.696, Test Loss: 1284435.500, Test R²: 0.224\n",
      "Epoch: 693, Train Loss: 524340.062, Train R²: 0.696, Test Loss: 1282584.000, Test R²: 0.225\n",
      "Epoch: 694, Train Loss: 523510.406, Train R²: 0.697, Test Loss: 1286408.875, Test R²: 0.223\n",
      "Epoch: 695, Train Loss: 522584.875, Train R²: 0.697, Test Loss: 1284372.250, Test R²: 0.224\n",
      "Epoch: 696, Train Loss: 521700.969, Train R²: 0.698, Test Loss: 1285860.375, Test R²: 0.223\n",
      "Epoch: 697, Train Loss: 520931.844, Train R²: 0.698, Test Loss: 1286814.250, Test R²: 0.222\n",
      "Epoch: 698, Train Loss: 520226.906, Train R²: 0.699, Test Loss: 1286040.625, Test R²: 0.223\n",
      "Epoch: 699, Train Loss: 519510.281, Train R²: 0.699, Test Loss: 1289594.125, Test R²: 0.221\n",
      "Epoch: 700, Train Loss: 518736.625, Train R²: 0.699, Test Loss: 1287741.250, Test R²: 0.222\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.metrics import r2_score\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils import set_seed\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 5\n",
    "set_seed(SEED)\n",
    "\n",
    "# 假定 featureDF, ppi_matrix, 和 all_sequence_outputsnew 已经定义好\n",
    "\n",
    "# 转换为PyTorch Tensor\n",
    "edge_index = torch.tensor(np.array([ppi_matrix.row, ppi_matrix.col]), dtype=torch.long)\n",
    "\n",
    "# 创建PyTorch Geometric的Data对象\n",
    "graph = Data(\n",
    "    x=torch.tensor(merged_df['rNC2'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(merged_df['NC3'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    seq=torch.tensor(all_sequence_outputsnew, dtype=torch.float32),\n",
    "    pause=torch.tensor(merged_df['High_Pause_Counts'].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    ")\n",
    "\n",
    "# 划分训练集和测试集\n",
    "split = T.RandomNodeSplit(num_test=0.1)\n",
    "graph = split(graph)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 32)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9216, 32),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 32),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.conv1 = SAGEConv(32, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        seq_embedding = data.seq\n",
    "        edge_index = data.edge_index\n",
    "        pausescore = data.pause\n",
    "\n",
    "        x = self.fc(x) + self.encoder(seq_embedding)\n",
    "        x = torch.cat((self.fc(pausescore), x), dim=1)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net = NeuralNet().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 700\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    neural_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 获取原始特征和掩码特征\n",
    "    out = neural_net(graph)\n",
    "    train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = neural_net(graph)\n",
    "            train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "            train_r2 = r2_score(graph.y[graph.train_mask].cpu().numpy(), out[graph.train_mask].cpu().numpy())\n",
    "\n",
    "            test_loss = criterion(out[graph.test_mask], graph.y[graph.test_mask])\n",
    "            test_r2 = r2_score(graph.y[graph.test_mask].cpu().numpy(), out[graph.test_mask].cpu().numpy())\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, Train R²: {train_r2:.3f}, Test Loss: {test_loss:.3f}, Test R²: {test_r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 5\n",
      "Epoch: 020, Train Loss: 1215026.375, Train R²: 0.287, Test Loss: 984010.500, Test R²: 0.342\n",
      "Epoch: 040, Train Loss: 1099386.500, Train R²: 0.355, Test Loss: 865000.625, Test R²: 0.422\n",
      "Epoch: 060, Train Loss: 1093466.750, Train R²: 0.359, Test Loss: 869298.438, Test R²: 0.419\n",
      "Epoch: 080, Train Loss: 1089486.500, Train R²: 0.361, Test Loss: 869460.375, Test R²: 0.419\n",
      "Epoch: 100, Train Loss: 1084752.000, Train R²: 0.364, Test Loss: 865629.438, Test R²: 0.421\n",
      "Epoch: 120, Train Loss: 1078264.500, Train R²: 0.368, Test Loss: 864248.562, Test R²: 0.422\n",
      "Epoch: 140, Train Loss: 1069508.000, Train R²: 0.373, Test Loss: 863702.688, Test R²: 0.423\n",
      "Epoch: 160, Train Loss: 1058182.250, Train R²: 0.379, Test Loss: 864331.375, Test R²: 0.422\n",
      "Epoch: 180, Train Loss: 1044473.375, Train R²: 0.387, Test Loss: 867390.125, Test R²: 0.420\n",
      "Epoch: 200, Train Loss: 1028420.562, Train R²: 0.397, Test Loss: 873237.438, Test R²: 0.416\n",
      "Epoch: 220, Train Loss: 1010177.500, Train R²: 0.408, Test Loss: 883538.062, Test R²: 0.409\n",
      "Epoch: 240, Train Loss: 988359.562, Train R²: 0.420, Test Loss: 894411.062, Test R²: 0.402\n",
      "Epoch: 260, Train Loss: 963315.062, Train R²: 0.435, Test Loss: 904037.250, Test R²: 0.396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m neural_net(graph)\n\u001b[1;32m     88\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(out[graph\u001b[38;5;241m.\u001b[39mtrain_mask], graph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import r2_score\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils import set_seed\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 5\n",
    "set_seed(SEED)\n",
    "\n",
    "# 假定 featureDF, ppi_matrix, 和 all_sequence_outputsnew 已经定义好\n",
    "\n",
    "# 转换为PyTorch Tensor\n",
    "edge_index = torch.tensor(np.array([ppi_matrix.row, ppi_matrix.col]), dtype=torch.long)\n",
    "\n",
    "# 创建PyTorch Geometric的Data对象\n",
    "graph = Data(\n",
    "    x=torch.tensor(merged_df['rNC2'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(merged_df['NC3'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    seq=torch.tensor(all_sequence_outputsnew, dtype=torch.float32),\n",
    "    pause=torch.tensor(merged_df['High_Pause_Counts'].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    ")\n",
    "\n",
    "# 划分训练集、验证集和测试集#这个和上面划分效果差不多，测试过了\n",
    "split = T.RandomNodeSplit(num_val=0.0, num_test=0.1)\n",
    "graph = split(graph)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 32)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9216, 32),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.conv1 = GCNConv(64, 1)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        seq_embedding = data.seq\n",
    "        edge_index = data.edge_index\n",
    "        pausescore = data.pause\n",
    "\n",
    "        x = self.fc(x) + self.encoder(seq_embedding)\n",
    "        x = torch.cat((self.fc(pausescore), x), dim=1)\n",
    "        x = F.gelu(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        x = self.fc1(x)\n",
    "        #print(f'final output {x.shape}')\n",
    "\n",
    "    \n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net = NeuralNet().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000000\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    neural_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 获取原始特征和掩码特征\n",
    "    out = neural_net(graph)\n",
    "    train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch %20 == 0:\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = neural_net(graph)\n",
    "            train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "            train_r2 = r2_score(graph.y[graph.train_mask].cpu().numpy(), out[graph.train_mask].cpu().numpy())\n",
    "\n",
    "            # val_loss = criterion(out[graph.val_mask], graph.y[graph.val_mask])\n",
    "            # val_r2 = r2_score(graph.y[graph.val_mask].cpu().numpy(), out[graph.val_mask].cpu().numpy())\n",
    "\n",
    "            test_loss = criterion(out[graph.test_mask], graph.y[graph.test_mask])\n",
    "            test_r2 = r2_score(graph.y[graph.test_mask].cpu().numpy(), out[graph.test_mask].cpu().numpy())\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, Train R²: {train_r2:.3f}, Test Loss: {test_loss:.3f}, Test R²: {test_r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加上权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 5\n",
      "Epoch: 020, Train Loss: 2063961.250, Train R²: -0.210, Test Loss: 1912968.125, Test R²: -0.279\n",
      "Epoch: 040, Train Loss: 1963424.375, Train R²: -0.151, Test Loss: 1838486.625, Test R²: -0.229\n",
      "Epoch: 060, Train Loss: 1915329.250, Train R²: -0.123, Test Loss: 1803173.500, Test R²: -0.205\n",
      "Epoch: 080, Train Loss: 1878073.625, Train R²: -0.101, Test Loss: 1777382.500, Test R²: -0.188\n",
      "Epoch: 100, Train Loss: 1847095.875, Train R²: -0.083, Test Loss: 1753783.125, Test R²: -0.172\n",
      "Epoch: 120, Train Loss: 1825744.750, Train R²: -0.071, Test Loss: 1735472.625, Test R²: -0.160\n",
      "Epoch: 140, Train Loss: 1813823.750, Train R²: -0.064, Test Loss: 1723657.375, Test R²: -0.152\n",
      "Epoch: 160, Train Loss: 1805512.875, Train R²: -0.059, Test Loss: 1715047.750, Test R²: -0.146\n",
      "Epoch: 180, Train Loss: 1799773.750, Train R²: -0.055, Test Loss: 1709228.375, Test R²: -0.143\n",
      "Epoch: 200, Train Loss: 1795655.750, Train R²: -0.053, Test Loss: 1705703.000, Test R²: -0.140\n",
      "Epoch: 220, Train Loss: 1792116.750, Train R²: -0.051, Test Loss: 1704178.375, Test R²: -0.139\n",
      "Epoch: 240, Train Loss: 1788864.625, Train R²: -0.049, Test Loss: 1703467.125, Test R²: -0.139\n",
      "Epoch: 260, Train Loss: 1785805.500, Train R²: -0.047, Test Loss: 1703090.750, Test R²: -0.138\n",
      "Epoch: 280, Train Loss: 1782778.750, Train R²: -0.046, Test Loss: 1703095.875, Test R²: -0.138\n",
      "Epoch: 300, Train Loss: 1777561.250, Train R²: -0.042, Test Loss: 1702572.375, Test R²: -0.138\n",
      "Epoch: 320, Train Loss: 1765777.750, Train R²: -0.036, Test Loss: 1698903.625, Test R²: -0.136\n",
      "Epoch: 340, Train Loss: 1754603.875, Train R²: -0.029, Test Loss: 1694291.250, Test R²: -0.133\n",
      "Epoch: 360, Train Loss: 1745057.000, Train R²: -0.023, Test Loss: 1690047.125, Test R²: -0.130\n",
      "Epoch: 380, Train Loss: 1736080.000, Train R²: -0.018, Test Loss: 1686369.625, Test R²: -0.127\n",
      "Epoch: 400, Train Loss: 1727197.250, Train R²: -0.013, Test Loss: 1681562.250, Test R²: -0.124\n",
      "Epoch: 420, Train Loss: 1718377.000, Train R²: -0.008, Test Loss: 1679682.750, Test R²: -0.123\n",
      "Epoch: 440, Train Loss: 1706208.125, Train R²: -0.001, Test Loss: 1676072.500, Test R²: -0.120\n",
      "Epoch: 460, Train Loss: 1693642.000, Train R²: 0.007, Test Loss: 1673961.000, Test R²: -0.119\n",
      "Epoch: 480, Train Loss: 1680670.875, Train R²: 0.014, Test Loss: 1672924.500, Test R²: -0.118\n",
      "Epoch: 500, Train Loss: 1667399.625, Train R²: 0.022, Test Loss: 1673937.125, Test R²: -0.119\n",
      "Epoch: 520, Train Loss: 1653294.500, Train R²: 0.030, Test Loss: 1676817.625, Test R²: -0.121\n",
      "Epoch: 540, Train Loss: 1638404.625, Train R²: 0.039, Test Loss: 1681603.250, Test R²: -0.124\n",
      "Epoch: 560, Train Loss: 1623918.500, Train R²: 0.048, Test Loss: 1684533.875, Test R²: -0.126\n",
      "Epoch: 580, Train Loss: 1610814.375, Train R²: 0.055, Test Loss: 1689617.625, Test R²: -0.129\n",
      "Epoch: 600, Train Loss: 1598732.625, Train R²: 0.062, Test Loss: 1695935.375, Test R²: -0.134\n",
      "Epoch: 620, Train Loss: 1587230.375, Train R²: 0.069, Test Loss: 1704227.500, Test R²: -0.139\n",
      "Epoch: 640, Train Loss: 1575782.375, Train R²: 0.076, Test Loss: 1715773.000, Test R²: -0.147\n",
      "Epoch: 660, Train Loss: 1564571.875, Train R²: 0.082, Test Loss: 1724841.125, Test R²: -0.153\n",
      "Epoch: 680, Train Loss: 1553997.625, Train R²: 0.089, Test Loss: 1740162.375, Test R²: -0.163\n",
      "Epoch: 700, Train Loss: 1544128.250, Train R²: 0.094, Test Loss: 1755347.500, Test R²: -0.173\n",
      "Epoch: 720, Train Loss: 1534520.875, Train R²: 0.100, Test Loss: 1769760.875, Test R²: -0.183\n",
      "Epoch: 740, Train Loss: 1525062.375, Train R²: 0.106, Test Loss: 1782547.375, Test R²: -0.192\n",
      "Epoch: 760, Train Loss: 1515025.125, Train R²: 0.112, Test Loss: 1793366.375, Test R²: -0.199\n",
      "Epoch: 780, Train Loss: 1505268.375, Train R²: 0.117, Test Loss: 1801084.875, Test R²: -0.204\n",
      "Epoch: 800, Train Loss: 1495883.875, Train R²: 0.123, Test Loss: 1806746.000, Test R²: -0.208\n",
      "Epoch: 820, Train Loss: 1487007.250, Train R²: 0.128, Test Loss: 1813191.250, Test R²: -0.212\n",
      "Epoch: 840, Train Loss: 1478440.625, Train R²: 0.133, Test Loss: 1817077.625, Test R²: -0.215\n",
      "Epoch: 860, Train Loss: 1470498.500, Train R²: 0.138, Test Loss: 1820967.500, Test R²: -0.217\n",
      "Epoch: 880, Train Loss: 1462217.125, Train R²: 0.142, Test Loss: 1818864.000, Test R²: -0.216\n",
      "Epoch: 900, Train Loss: 1454441.000, Train R²: 0.147, Test Loss: 1821212.750, Test R²: -0.217\n",
      "Epoch: 920, Train Loss: 1446997.375, Train R²: 0.151, Test Loss: 1821295.875, Test R²: -0.217\n",
      "Epoch: 940, Train Loss: 1439899.750, Train R²: 0.156, Test Loss: 1820521.375, Test R²: -0.217\n",
      "Epoch: 960, Train Loss: 1433010.250, Train R²: 0.160, Test Loss: 1817217.000, Test R²: -0.215\n",
      "Epoch: 980, Train Loss: 1426009.375, Train R²: 0.164, Test Loss: 1810112.625, Test R²: -0.210\n",
      "Epoch: 1000, Train Loss: 1419614.000, Train R²: 0.167, Test Loss: 1805697.750, Test R²: -0.207\n",
      "Epoch: 1020, Train Loss: 1413790.375, Train R²: 0.171, Test Loss: 1800863.000, Test R²: -0.204\n",
      "Epoch: 1040, Train Loss: 1407955.625, Train R²: 0.174, Test Loss: 1795411.625, Test R²: -0.200\n",
      "Epoch: 1060, Train Loss: 1402497.125, Train R²: 0.178, Test Loss: 1785594.250, Test R²: -0.194\n",
      "Epoch: 1080, Train Loss: 1397410.125, Train R²: 0.180, Test Loss: 1771812.625, Test R²: -0.184\n",
      "Epoch: 1100, Train Loss: 1392489.750, Train R²: 0.183, Test Loss: 1757854.500, Test R²: -0.175\n",
      "Epoch: 1120, Train Loss: 1387296.250, Train R²: 0.186, Test Loss: 1749146.375, Test R²: -0.169\n",
      "Epoch: 1140, Train Loss: 1382705.000, Train R²: 0.189, Test Loss: 1739706.750, Test R²: -0.163\n",
      "Epoch: 1160, Train Loss: 1378216.000, Train R²: 0.192, Test Loss: 1730119.125, Test R²: -0.157\n",
      "Epoch: 1180, Train Loss: 1373756.500, Train R²: 0.194, Test Loss: 1720611.500, Test R²: -0.150\n",
      "Epoch: 1200, Train Loss: 1369241.375, Train R²: 0.197, Test Loss: 1713923.250, Test R²: -0.146\n",
      "Epoch: 1220, Train Loss: 1365119.625, Train R²: 0.199, Test Loss: 1707030.375, Test R²: -0.141\n",
      "Epoch: 1240, Train Loss: 1361104.250, Train R²: 0.202, Test Loss: 1698251.250, Test R²: -0.135\n",
      "Epoch: 1260, Train Loss: 1357045.625, Train R²: 0.204, Test Loss: 1692355.000, Test R²: -0.131\n",
      "Epoch: 1280, Train Loss: 1353400.000, Train R²: 0.206, Test Loss: 1685282.500, Test R²: -0.127\n",
      "Epoch: 1300, Train Loss: 1349501.375, Train R²: 0.209, Test Loss: 1681112.875, Test R²: -0.124\n",
      "Epoch: 1320, Train Loss: 1346003.625, Train R²: 0.211, Test Loss: 1677249.125, Test R²: -0.121\n",
      "Epoch: 1340, Train Loss: 1342496.750, Train R²: 0.213, Test Loss: 1672488.500, Test R²: -0.118\n",
      "Epoch: 1360, Train Loss: 1339077.000, Train R²: 0.215, Test Loss: 1667067.625, Test R²: -0.114\n",
      "Epoch: 1380, Train Loss: 1335967.500, Train R²: 0.217, Test Loss: 1662088.125, Test R²: -0.111\n",
      "Epoch: 1400, Train Loss: 1332796.375, Train R²: 0.218, Test Loss: 1659967.500, Test R²: -0.110\n",
      "Epoch: 1420, Train Loss: 1329101.000, Train R²: 0.221, Test Loss: 1661066.375, Test R²: -0.110\n",
      "Epoch: 1440, Train Loss: 1325864.875, Train R²: 0.222, Test Loss: 1661207.000, Test R²: -0.110\n",
      "Epoch: 1460, Train Loss: 1322992.000, Train R²: 0.224, Test Loss: 1663972.750, Test R²: -0.112\n",
      "Epoch: 1480, Train Loss: 1319592.125, Train R²: 0.226, Test Loss: 1661896.750, Test R²: -0.111\n",
      "Epoch: 1500, Train Loss: 1317072.625, Train R²: 0.228, Test Loss: 1659714.000, Test R²: -0.109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 获取原始特征和掩码特征\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mneural_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(out[graph\u001b[38;5;241m.\u001b[39mtrain_mask], graph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     89\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 66\u001b[0m, in \u001b[0;36mNeuralNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(pausescore), x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[0;32m---> 66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#x = self.fc1(x)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/mnt/md0/luying/anaconda3/envs/pyg/lib/python3.8/site-packages/torch_geometric/utils/loop.py:641\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    637\u001b[0m     loop_attr \u001b[38;5;241m=\u001b[39m compute_loop_attr(  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    638\u001b[0m         edge_index, edge_attr, N, \u001b[38;5;28;01mFalse\u001b[39;00m, fill_value)\n\u001b[1;32m    640\u001b[0m     inv_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mmask\n\u001b[0;32m--> 641\u001b[0m     loop_attr[\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43minv_mask\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    643\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    645\u001b[0m is_undirected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import r2_score\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils import set_seed\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 5\n",
    "set_seed(SEED)\n",
    "\n",
    "# 假定 featureDF, ppi_matrix, 和 all_sequence_outputsnew 已经定义好\n",
    "\n",
    "# 转换为PyTorch Tensor\n",
    "edge_index = torch.tensor(np.array([ppi_matrix.row, ppi_matrix.col]), dtype=torch.long)\n",
    "edge_weight = torch.tensor(ppi_matrix.data, dtype=torch.float32)\n",
    "\n",
    "# 创建PyTorch Geometric的Data对象\n",
    "graph = Data(\n",
    "    x=torch.tensor(merged_df['rNC2'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    edge_index=edge_index,\n",
    "    edge_weight=edge_weight,\n",
    "    y=torch.tensor(merged_df['NC3'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    seq=torch.tensor(all_sequence_outputsnew, dtype=torch.float32),\n",
    "    pause=torch.tensor(merged_df['High_Pause_Counts'].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    ")\n",
    "\n",
    "# 划分训练集、验证集和测试集#这个和上面划分效果差不多，测试过了\n",
    "split = T.RandomNodeSplit(num_val=0.0, num_test=0.1)\n",
    "graph = split(graph)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 32)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9216, 32),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.conv1 = GCNConv(64, 1)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        seq_embedding = data.seq\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = data.edge_weight\n",
    "        pausescore = data.pause\n",
    "\n",
    "        x = self.fc(x) + self.encoder(seq_embedding)\n",
    "        x = torch.cat((self.fc(pausescore), x), dim=1)\n",
    "        x = F.gelu(x)\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        #x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net = NeuralNet().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000000\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    neural_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 获取原始特征和掩码特征\n",
    "    out = neural_net(graph)\n",
    "    train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = neural_net(graph)\n",
    "            train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "            train_r2 = r2_score(graph.y[graph.train_mask].cpu().numpy(), out[graph.train_mask].cpu().numpy())\n",
    "\n",
    "            # val_loss = criterion(out[graph.val_mask], graph.y[graph.val_mask])\n",
    "            # val_r2 = r2_score(graph.y[graph.val_mask].cpu().numpy(), out[graph.val_mask].cpu().numpy())\n",
    "\n",
    "            test_loss = criterion(out[graph.test_mask], graph.y[graph.test_mask])\n",
    "            test_r2 = r2_score(graph.y[graph.test_mask].cpu().numpy(), out[graph.test_mask].cpu().numpy())\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, Train R²: {train_r2:.3f}, Test Loss: {test_loss:.3f}, Test R²: {test_r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0       transcript_id            protein  rNC1  rNC2     NC1  \\\n",
      "0           1  ENST00000000233.10  ENSP00000000233.5   449   379  1451.3   \n",
      "1          18   ENST00000000412.8  ENSP00000000412.3   443   384   758.3   \n",
      "2          37   ENST00000001008.6  ENSP00000001008.4  1015   899  5355.8   \n",
      "3          62   ENST00000002125.9  ENSP00000002125.4   203   134   479.2   \n",
      "4          87  ENST00000002165.11  ENSP00000002165.5   905   750   556.8   \n",
      "\n",
      "      NC2     NC3         protein_id  High_Pause_Counts  ensp_id  \n",
      "0  1458.2  1561.2  ENSP00000000233.5                7.0   2453.0  \n",
      "1   817.2   815.6  ENSP00000000412.3                4.0   3604.0  \n",
      "2  5303.3  5393.0  ENSP00000001008.4                9.0   3579.0  \n",
      "3   562.1   563.5  ENSP00000002125.4               13.0    758.0  \n",
      "4   567.1   605.8  ENSP00000002165.5               16.0   2185.0  \n",
      "    Unnamed: 0        protein_id2        protein_id1  Protein1  Protein2  \\\n",
      "80          81  ENSP00000317992.6  ENSP00000219240.4     209.0    2292.0   \n",
      "84          85  ENSP00000317992.6  ENSP00000257347.4     714.0    2292.0   \n",
      "85          86  ENSP00000317992.6  ENSP00000258424.2     744.0    2292.0   \n",
      "86          87  ENSP00000317992.6  ENSP00000260270.2     789.0    2292.0   \n",
      "94          95  ENSP00000317992.6  ENSP00000295797.4    1665.0    2292.0   \n",
      "\n",
      "    CombinedScore  ensp_id2  ensp_id1  \n",
      "80            245         0      4650  \n",
      "84            470         0      4015  \n",
      "85            295         0       879  \n",
      "86            493         0      3518  \n",
      "94            152         0      1401  \n"
     ]
    }
   ],
   "source": [
    "unique_proteins = pd.unique(ppi[['protein_id2', 'protein_id1']].values.ravel('K'))\n",
    "protein_id_map = {protein: idx for idx, protein in enumerate(unique_proteins)}\n",
    "\n",
    "merged_df['ensp_id'] = merged_df['protein'].map(protein_id_map)\n",
    "ppi['ensp_id2'] = ppi['protein_id2'].map(protein_id_map)\n",
    "ppi['ensp_id1'] = ppi['protein_id1'].map(protein_id_map)\n",
    "print(merged_df.head())\n",
    "print(ppi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7132x7132 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 364726 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ppi直接加载构建好的npz，以下是构建npz步骤\n",
    "\n",
    "#用ppi构建一个coo的稀疏举证，用protein1和protein2构建一个coo的稀疏矩阵\n",
    "ppi_matrix = sp.coo_matrix((ppi[\"CombinedScore\"], (ppi['ensp_id2'], ppi['ensp_id1'])), shape=(merged_df.shape[0], merged_df.shape[0]))\n",
    "\n",
    "ppi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed = 5\n",
      "Epoch: 020, Train Loss: 1215026.375, Train R²: 0.287, Test Loss: 984010.500, Test R²: 0.342\n",
      "Epoch: 040, Train Loss: 1099386.500, Train R²: 0.355, Test Loss: 865000.625, Test R²: 0.422\n",
      "Epoch: 060, Train Loss: 1093466.750, Train R²: 0.359, Test Loss: 869298.438, Test R²: 0.419\n",
      "Epoch: 080, Train Loss: 1089486.500, Train R²: 0.361, Test Loss: 869460.375, Test R²: 0.419\n",
      "Epoch: 100, Train Loss: 1084752.000, Train R²: 0.364, Test Loss: 865629.438, Test R²: 0.421\n",
      "Epoch: 120, Train Loss: 1078264.500, Train R²: 0.368, Test Loss: 864248.562, Test R²: 0.422\n",
      "Epoch: 140, Train Loss: 1069508.000, Train R²: 0.373, Test Loss: 863702.688, Test R²: 0.423\n",
      "Epoch: 160, Train Loss: 1058182.250, Train R²: 0.379, Test Loss: 864331.375, Test R²: 0.422\n",
      "Epoch: 180, Train Loss: 1044473.375, Train R²: 0.387, Test Loss: 867390.125, Test R²: 0.420\n",
      "Epoch: 200, Train Loss: 1028420.562, Train R²: 0.397, Test Loss: 873237.438, Test R²: 0.416\n",
      "Epoch: 220, Train Loss: 1010177.500, Train R²: 0.408, Test Loss: 883538.062, Test R²: 0.409\n",
      "Epoch: 240, Train Loss: 988359.562, Train R²: 0.420, Test Loss: 894411.062, Test R²: 0.402\n",
      "Epoch: 260, Train Loss: 963315.062, Train R²: 0.435, Test Loss: 904037.250, Test R²: 0.396\n",
      "Epoch: 280, Train Loss: 924681.875, Train R²: 0.458, Test Loss: 924071.312, Test R²: 0.382\n",
      "Epoch: 300, Train Loss: 884691.688, Train R²: 0.481, Test Loss: 939781.188, Test R²: 0.372\n",
      "Epoch: 320, Train Loss: 841683.812, Train R²: 0.506, Test Loss: 956948.438, Test R²: 0.360\n",
      "Epoch: 340, Train Loss: 800260.812, Train R²: 0.531, Test Loss: 981253.438, Test R²: 0.344\n",
      "Epoch: 360, Train Loss: 756899.375, Train R²: 0.556, Test Loss: 1018518.438, Test R²: 0.319\n",
      "Epoch: 380, Train Loss: 716254.438, Train R²: 0.580, Test Loss: 1048106.875, Test R²: 0.299\n",
      "Epoch: 400, Train Loss: 680841.000, Train R²: 0.601, Test Loss: 1092766.750, Test R²: 0.270\n",
      "Epoch: 420, Train Loss: 643412.938, Train R²: 0.623, Test Loss: 1130416.250, Test R²: 0.244\n",
      "Epoch: 440, Train Loss: 614231.000, Train R²: 0.640, Test Loss: 1168333.500, Test R²: 0.219\n",
      "Epoch: 460, Train Loss: 587802.188, Train R²: 0.655, Test Loss: 1181006.250, Test R²: 0.211\n",
      "Epoch: 480, Train Loss: 563813.875, Train R²: 0.669, Test Loss: 1187507.125, Test R²: 0.206\n",
      "Epoch: 500, Train Loss: 540197.938, Train R²: 0.683, Test Loss: 1226355.375, Test R²: 0.180\n",
      "Epoch: 520, Train Loss: 520506.188, Train R²: 0.695, Test Loss: 1242663.375, Test R²: 0.169\n",
      "Epoch: 540, Train Loss: 503203.000, Train R²: 0.705, Test Loss: 1257500.875, Test R²: 0.159\n",
      "Epoch: 560, Train Loss: 490758.094, Train R²: 0.712, Test Loss: 1297659.000, Test R²: 0.133\n",
      "Epoch: 580, Train Loss: 471793.375, Train R²: 0.723, Test Loss: 1296453.625, Test R²: 0.133\n",
      "Epoch: 600, Train Loss: 457402.812, Train R²: 0.732, Test Loss: 1328955.375, Test R²: 0.112\n",
      "Epoch: 620, Train Loss: 444753.469, Train R²: 0.739, Test Loss: 1349486.875, Test R²: 0.098\n",
      "Epoch: 640, Train Loss: 434763.344, Train R²: 0.745, Test Loss: 1396677.625, Test R²: 0.066\n",
      "Epoch: 660, Train Loss: 422468.844, Train R²: 0.752, Test Loss: 1375429.875, Test R²: 0.081\n",
      "Epoch: 680, Train Loss: 406695.500, Train R²: 0.761, Test Loss: 1407587.625, Test R²: 0.059\n",
      "Epoch: 700, Train Loss: 397269.969, Train R²: 0.767, Test Loss: 1405343.500, Test R²: 0.061\n",
      "Epoch: 720, Train Loss: 391239.875, Train R²: 0.771, Test Loss: 1396558.125, Test R²: 0.066\n",
      "Epoch: 740, Train Loss: 380896.094, Train R²: 0.777, Test Loss: 1453453.750, Test R²: 0.028\n",
      "Epoch: 760, Train Loss: 369363.938, Train R²: 0.783, Test Loss: 1447478.250, Test R²: 0.032\n",
      "Epoch: 780, Train Loss: 358907.125, Train R²: 0.790, Test Loss: 1439163.250, Test R²: 0.038\n",
      "Epoch: 800, Train Loss: 354283.219, Train R²: 0.792, Test Loss: 1422461.625, Test R²: 0.049\n",
      "Epoch: 820, Train Loss: 349007.656, Train R²: 0.795, Test Loss: 1417802.250, Test R²: 0.052\n",
      "Epoch: 840, Train Loss: 339090.656, Train R²: 0.801, Test Loss: 1488233.500, Test R²: 0.005\n",
      "Epoch: 860, Train Loss: 333052.844, Train R²: 0.805, Test Loss: 1491265.125, Test R²: 0.003\n",
      "Epoch: 880, Train Loss: 329756.562, Train R²: 0.807, Test Loss: 1526306.250, Test R²: -0.020\n",
      "Epoch: 900, Train Loss: 318514.188, Train R²: 0.813, Test Loss: 1503450.500, Test R²: -0.005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 获取原始特征和掩码特征\u001b[39;00m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m neural_net(graph)\n\u001b[0;32m---> 88\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m, graph\u001b[38;5;241m.\u001b[39my[graph\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m     89\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.metrics import r2_score\n",
    "import torch_geometric.transforms as T\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.utils import set_seed\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 5\n",
    "set_seed(SEED)\n",
    "\n",
    "# 假定 featureDF, ppi_matrix, 和 all_sequence_outputsnew 已经定义好\n",
    "\n",
    "# 转换为PyTorch Tensor\n",
    "edge_index = torch.tensor(np.array([ppi_matrix.row, ppi_matrix.col]), dtype=torch.long)\n",
    "\n",
    "# 创建PyTorch Geometric的Data对象\n",
    "graph = Data(\n",
    "    x=torch.tensor(merged_df['rNC2'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    edge_index=edge_index,\n",
    "    y=torch.tensor(merged_df['NC3'].to_numpy(), dtype=torch.float32).unsqueeze(1),\n",
    "    seq=torch.tensor(all_sequence_outputsnew, dtype=torch.float32),\n",
    "    pause=torch.tensor(merged_df['High_Pause_Counts'].to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    ")\n",
    "\n",
    "# 划分训练集、验证集和测试集#这个和上面划分效果差不多，测试过了\n",
    "split = T.RandomNodeSplit(num_val=0.0, num_test=0.1)\n",
    "graph = split(graph)\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 32)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(9216, 32),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.conv1 = GCNConv(64, 1)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        seq_embedding = data.seq\n",
    "        edge_index = data.edge_index\n",
    "        pausescore = data.pause\n",
    "\n",
    "        x = self.fc(x) + self.encoder(seq_embedding)\n",
    "        x = torch.cat((self.fc(pausescore), x), dim=1)\n",
    "        x = F.gelu(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.conv1(x, edge_index)\n",
    "        x = self.fc1(x)\n",
    "        #print(f'final output {x.shape}')\n",
    "\n",
    "    \n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_net = NeuralNet().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "optimizer = optim.Adam(neural_net.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "num_epochs = 1000000\n",
    "\n",
    "graph = graph.to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    neural_net.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 获取原始特征和掩码特征\n",
    "    out = neural_net(graph)\n",
    "    train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch %20 == 0:\n",
    "        neural_net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = neural_net(graph)\n",
    "            train_loss = criterion(out[graph.train_mask], graph.y[graph.train_mask])\n",
    "            train_r2 = r2_score(graph.y[graph.train_mask].cpu().numpy(), out[graph.train_mask].cpu().numpy())\n",
    "\n",
    "            # val_loss = criterion(out[graph.val_mask], graph.y[graph.val_mask])\n",
    "            # val_r2 = r2_score(graph.y[graph.val_mask].cpu().numpy(), out[graph.val_mask].cpu().numpy())\n",
    "\n",
    "            test_loss = criterion(out[graph.test_mask], graph.y[graph.test_mask])\n",
    "            test_r2 = r2_score(graph.y[graph.test_mask].cpu().numpy(), out[graph.test_mask].cpu().numpy())\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, Train R²: {train_r2:.3f}, Test Loss: {test_loss:.3f}, Test R²: {test_r2:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
